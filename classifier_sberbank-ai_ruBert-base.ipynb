{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9yQRUIcYsMAf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "# transformers ver >= 4.8.2\n",
    "from transformers import BertTokenizer, BertForSequenceClassification \n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "29JyMfRaeAIv"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"text_series_full.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>трамп клинтон выиграли праймериз аризоне проиг...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Госдума утвердила введение присяги вступлении ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>черная дыра роскосмоса откуда берутся заоблачн...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>уганде родители пытались убить дочь переход ис...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Белгородский захватчик банка получил года коло...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>будущей верховной раде видны контуры правящей ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>крах вимавиа эксперты дали советы клиентам оск...</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>очень интересное затмение прошедшее солнечное ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>меркель оценила сдержанную реакцию украины вво...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>протест ученых против кафедры теологии мифи св...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     class\n",
       "0  трамп клинтон выиграли праймериз аризоне проиг...     world\n",
       "1  Госдума утвердила введение присяги вступлении ...  politics\n",
       "2  черная дыра роскосмоса откуда берутся заоблачн...  politics\n",
       "3  уганде родители пытались убить дочь переход ис...  religion\n",
       "4  Белгородский захватчик банка получил года коло...  politics\n",
       "5  будущей верховной раде видны контуры правящей ...  politics\n",
       "6  крах вимавиа эксперты дали советы клиентам оск...   society\n",
       "7  очень интересное затмение прошедшее солнечное ...   science\n",
       "8  меркель оценила сдержанную реакцию украины вво...     world\n",
       "9  протест ученых против кафедры теологии мифи св...  religion"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['text', 'class']]\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(x.split(' ')[:10]))\n",
    "df = df.dropna()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407584, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    162184\n",
       "2    113942\n",
       "5     51555\n",
       "4     45519\n",
       "3     22518\n",
       "0      6610\n",
       "1      5256\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем предсказывать typeid\n",
    "\n",
    "texts = df['text'].values\n",
    "labels = df['class'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Скачиваем токенизатор ruBert-base\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('sberbank-ai/ruBert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d545d488e1c4aafb613b848fa62baab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=407584.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:  трамп клинтон выиграли праймериз аризоне проиграли кокусы юте айдахо трамп\n",
      "Token IDs: tensor([   101,  11166,    388,  22061,   2297,  19672,    925,   1622,  54174,\n",
      "           396, 115556,  16630,  23766, 115012,    974,  87707,    376, 110183,\n",
      "          3830,    375,  11166,    388,    102,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "# Токенизируем все names\n",
    "\n",
    "def encode(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 32,          \n",
    "                        pad_to_max_length = True,\n",
    "                        return_tensors = 'pt',\n",
    "                        truncation = True,\n",
    "                        padding='max_length'\n",
    "                   )\n",
    "    \n",
    "    return encoded_dict['input_ids']\n",
    "    \n",
    "input_ids = list(map(lambda text: encode(text), tqdm(texts)))\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_ids, 'input_ids.pt')\n",
    "torch.save(labels, 'labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1ycdJ-q_sSNq"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.load('input_ids')\n",
    "labels = torch.load('labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EKPDNrnWmuNb"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J6W1wXO_mvLr"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3IeSFwomyas",
    "outputId": "0cff9f92-505d-491c-ba08-ef092966229d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/ruBert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"sberbank-ai/ruBert-base\",\n",
    "    num_labels = 7\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EBA3hDoCyrTN"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 3e-5,\n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_MfkT1_5ysHv"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3OFfg_t8yudN"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hScuqyLByxVl"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiTM-ODuyx-N",
    "outputId": "1e37fb30-4297-4932-cdce-0722d597a106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  8,916.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  8,916.    Elapsed: 0:00:18.\n",
      "  Batch   120  of  8,916.    Elapsed: 0:00:27.\n",
      "  Batch   160  of  8,916.    Elapsed: 0:00:36.\n",
      "  Batch   200  of  8,916.    Elapsed: 0:00:45.\n",
      "  Batch   240  of  8,916.    Elapsed: 0:00:54.\n",
      "  Batch   280  of  8,916.    Elapsed: 0:01:03.\n",
      "  Batch   320  of  8,916.    Elapsed: 0:01:12.\n",
      "  Batch   360  of  8,916.    Elapsed: 0:01:21.\n",
      "  Batch   400  of  8,916.    Elapsed: 0:01:31.\n",
      "  Batch   440  of  8,916.    Elapsed: 0:01:40.\n",
      "  Batch   480  of  8,916.    Elapsed: 0:01:49.\n",
      "  Batch   520  of  8,916.    Elapsed: 0:01:58.\n",
      "  Batch   560  of  8,916.    Elapsed: 0:02:07.\n",
      "  Batch   600  of  8,916.    Elapsed: 0:02:17.\n",
      "  Batch   640  of  8,916.    Elapsed: 0:02:26.\n",
      "  Batch   680  of  8,916.    Elapsed: 0:02:35.\n",
      "  Batch   720  of  8,916.    Elapsed: 0:02:44.\n",
      "  Batch   760  of  8,916.    Elapsed: 0:02:54.\n",
      "  Batch   800  of  8,916.    Elapsed: 0:03:03.\n",
      "  Batch   840  of  8,916.    Elapsed: 0:03:12.\n",
      "  Batch   880  of  8,916.    Elapsed: 0:03:21.\n",
      "  Batch   920  of  8,916.    Elapsed: 0:03:31.\n",
      "  Batch   960  of  8,916.    Elapsed: 0:03:40.\n",
      "  Batch 1,000  of  8,916.    Elapsed: 0:03:49.\n",
      "  Batch 1,040  of  8,916.    Elapsed: 0:03:58.\n",
      "  Batch 1,080  of  8,916.    Elapsed: 0:04:08.\n",
      "  Batch 1,120  of  8,916.    Elapsed: 0:04:17.\n",
      "  Batch 1,160  of  8,916.    Elapsed: 0:04:26.\n",
      "  Batch 1,200  of  8,916.    Elapsed: 0:04:35.\n",
      "  Batch 1,240  of  8,916.    Elapsed: 0:04:45.\n",
      "  Batch 1,280  of  8,916.    Elapsed: 0:04:54.\n",
      "  Batch 1,320  of  8,916.    Elapsed: 0:05:03.\n",
      "  Batch 1,360  of  8,916.    Elapsed: 0:05:12.\n",
      "  Batch 1,400  of  8,916.    Elapsed: 0:05:22.\n",
      "  Batch 1,440  of  8,916.    Elapsed: 0:05:31.\n",
      "  Batch 1,480  of  8,916.    Elapsed: 0:05:40.\n",
      "  Batch 1,520  of  8,916.    Elapsed: 0:05:49.\n",
      "  Batch 1,560  of  8,916.    Elapsed: 0:05:59.\n",
      "  Batch 1,600  of  8,916.    Elapsed: 0:06:08.\n",
      "  Batch 1,640  of  8,916.    Elapsed: 0:06:17.\n",
      "  Batch 1,680  of  8,916.    Elapsed: 0:06:26.\n",
      "  Batch 1,720  of  8,916.    Elapsed: 0:06:36.\n",
      "  Batch 1,760  of  8,916.    Elapsed: 0:06:45.\n",
      "  Batch 1,800  of  8,916.    Elapsed: 0:06:54.\n",
      "  Batch 1,840  of  8,916.    Elapsed: 0:07:04.\n",
      "  Batch 1,880  of  8,916.    Elapsed: 0:07:13.\n",
      "  Batch 1,920  of  8,916.    Elapsed: 0:07:22.\n",
      "  Batch 1,960  of  8,916.    Elapsed: 0:07:31.\n",
      "  Batch 2,000  of  8,916.    Elapsed: 0:07:41.\n",
      "  Batch 2,040  of  8,916.    Elapsed: 0:07:50.\n",
      "  Batch 2,080  of  8,916.    Elapsed: 0:07:59.\n",
      "  Batch 2,120  of  8,916.    Elapsed: 0:08:08.\n",
      "  Batch 2,160  of  8,916.    Elapsed: 0:08:18.\n",
      "  Batch 2,200  of  8,916.    Elapsed: 0:08:27.\n",
      "  Batch 2,240  of  8,916.    Elapsed: 0:08:36.\n",
      "  Batch 2,280  of  8,916.    Elapsed: 0:08:45.\n",
      "  Batch 2,320  of  8,916.    Elapsed: 0:08:55.\n",
      "  Batch 2,360  of  8,916.    Elapsed: 0:09:04.\n",
      "  Batch 2,400  of  8,916.    Elapsed: 0:09:13.\n",
      "  Batch 2,440  of  8,916.    Elapsed: 0:09:22.\n",
      "  Batch 2,480  of  8,916.    Elapsed: 0:09:32.\n",
      "  Batch 2,520  of  8,916.    Elapsed: 0:09:41.\n",
      "  Batch 2,560  of  8,916.    Elapsed: 0:09:50.\n",
      "  Batch 2,600  of  8,916.    Elapsed: 0:10:00.\n",
      "  Batch 2,640  of  8,916.    Elapsed: 0:10:09.\n",
      "  Batch 2,680  of  8,916.    Elapsed: 0:10:18.\n",
      "  Batch 2,720  of  8,916.    Elapsed: 0:10:27.\n",
      "  Batch 2,760  of  8,916.    Elapsed: 0:10:37.\n",
      "  Batch 2,800  of  8,916.    Elapsed: 0:10:46.\n",
      "  Batch 2,840  of  8,916.    Elapsed: 0:10:55.\n",
      "  Batch 2,880  of  8,916.    Elapsed: 0:11:04.\n",
      "  Batch 2,920  of  8,916.    Elapsed: 0:11:14.\n",
      "  Batch 2,960  of  8,916.    Elapsed: 0:11:23.\n",
      "  Batch 3,000  of  8,916.    Elapsed: 0:11:32.\n",
      "  Batch 3,040  of  8,916.    Elapsed: 0:11:42.\n",
      "  Batch 3,080  of  8,916.    Elapsed: 0:11:51.\n",
      "  Batch 3,120  of  8,916.    Elapsed: 0:12:00.\n",
      "  Batch 3,160  of  8,916.    Elapsed: 0:12:09.\n",
      "  Batch 3,200  of  8,916.    Elapsed: 0:12:19.\n",
      "  Batch 3,240  of  8,916.    Elapsed: 0:12:28.\n",
      "  Batch 3,280  of  8,916.    Elapsed: 0:12:37.\n",
      "  Batch 3,320  of  8,916.    Elapsed: 0:12:46.\n",
      "  Batch 3,360  of  8,916.    Elapsed: 0:12:56.\n",
      "  Batch 3,400  of  8,916.    Elapsed: 0:13:05.\n",
      "  Batch 3,440  of  8,916.    Elapsed: 0:13:14.\n",
      "  Batch 3,480  of  8,916.    Elapsed: 0:13:23.\n",
      "  Batch 3,520  of  8,916.    Elapsed: 0:13:33.\n",
      "  Batch 3,560  of  8,916.    Elapsed: 0:13:42.\n",
      "  Batch 3,600  of  8,916.    Elapsed: 0:13:51.\n",
      "  Batch 3,640  of  8,916.    Elapsed: 0:14:01.\n",
      "  Batch 3,680  of  8,916.    Elapsed: 0:14:10.\n",
      "  Batch 3,720  of  8,916.    Elapsed: 0:14:19.\n",
      "  Batch 3,760  of  8,916.    Elapsed: 0:14:28.\n",
      "  Batch 3,800  of  8,916.    Elapsed: 0:14:38.\n",
      "  Batch 3,840  of  8,916.    Elapsed: 0:14:47.\n",
      "  Batch 3,880  of  8,916.    Elapsed: 0:14:56.\n",
      "  Batch 3,920  of  8,916.    Elapsed: 0:15:05.\n",
      "  Batch 3,960  of  8,916.    Elapsed: 0:15:15.\n",
      "  Batch 4,000  of  8,916.    Elapsed: 0:15:24.\n",
      "  Batch 4,040  of  8,916.    Elapsed: 0:15:33.\n",
      "  Batch 4,080  of  8,916.    Elapsed: 0:15:42.\n",
      "  Batch 4,120  of  8,916.    Elapsed: 0:15:52.\n",
      "  Batch 4,160  of  8,916.    Elapsed: 0:16:01.\n",
      "  Batch 4,200  of  8,916.    Elapsed: 0:16:10.\n",
      "  Batch 4,240  of  8,916.    Elapsed: 0:16:20.\n",
      "  Batch 4,280  of  8,916.    Elapsed: 0:16:29.\n",
      "  Batch 4,320  of  8,916.    Elapsed: 0:16:38.\n",
      "  Batch 4,360  of  8,916.    Elapsed: 0:16:47.\n",
      "  Batch 4,400  of  8,916.    Elapsed: 0:16:57.\n",
      "  Batch 4,440  of  8,916.    Elapsed: 0:17:06.\n",
      "  Batch 4,480  of  8,916.    Elapsed: 0:17:15.\n",
      "  Batch 4,520  of  8,916.    Elapsed: 0:17:24.\n",
      "  Batch 4,560  of  8,916.    Elapsed: 0:17:34.\n",
      "  Batch 4,600  of  8,916.    Elapsed: 0:17:43.\n",
      "  Batch 4,640  of  8,916.    Elapsed: 0:17:52.\n",
      "  Batch 4,680  of  8,916.    Elapsed: 0:18:01.\n",
      "  Batch 4,720  of  8,916.    Elapsed: 0:18:11.\n",
      "  Batch 4,760  of  8,916.    Elapsed: 0:18:20.\n",
      "  Batch 4,800  of  8,916.    Elapsed: 0:18:29.\n",
      "  Batch 4,840  of  8,916.    Elapsed: 0:18:39.\n",
      "  Batch 4,880  of  8,916.    Elapsed: 0:18:48.\n",
      "  Batch 4,920  of  8,916.    Elapsed: 0:18:57.\n",
      "  Batch 4,960  of  8,916.    Elapsed: 0:19:06.\n",
      "  Batch 5,000  of  8,916.    Elapsed: 0:19:16.\n",
      "  Batch 5,040  of  8,916.    Elapsed: 0:19:25.\n",
      "  Batch 5,080  of  8,916.    Elapsed: 0:19:34.\n",
      "  Batch 5,120  of  8,916.    Elapsed: 0:19:44.\n",
      "  Batch 5,160  of  8,916.    Elapsed: 0:19:53.\n",
      "  Batch 5,200  of  8,916.    Elapsed: 0:20:02.\n",
      "  Batch 5,240  of  8,916.    Elapsed: 0:20:11.\n",
      "  Batch 5,280  of  8,916.    Elapsed: 0:20:21.\n",
      "  Batch 5,320  of  8,916.    Elapsed: 0:20:30.\n",
      "  Batch 5,360  of  8,916.    Elapsed: 0:20:39.\n",
      "  Batch 5,400  of  8,916.    Elapsed: 0:20:48.\n",
      "  Batch 5,440  of  8,916.    Elapsed: 0:20:58.\n",
      "  Batch 5,480  of  8,916.    Elapsed: 0:21:07.\n",
      "  Batch 5,520  of  8,916.    Elapsed: 0:21:16.\n",
      "  Batch 5,560  of  8,916.    Elapsed: 0:21:25.\n",
      "  Batch 5,600  of  8,916.    Elapsed: 0:21:35.\n",
      "  Batch 5,640  of  8,916.    Elapsed: 0:21:44.\n",
      "  Batch 5,680  of  8,916.    Elapsed: 0:21:53.\n",
      "  Batch 5,720  of  8,916.    Elapsed: 0:22:03.\n",
      "  Batch 5,760  of  8,916.    Elapsed: 0:22:12.\n",
      "  Batch 5,800  of  8,916.    Elapsed: 0:22:21.\n",
      "  Batch 5,840  of  8,916.    Elapsed: 0:22:30.\n",
      "  Batch 5,880  of  8,916.    Elapsed: 0:22:40.\n",
      "  Batch 5,920  of  8,916.    Elapsed: 0:22:49.\n",
      "  Batch 5,960  of  8,916.    Elapsed: 0:22:58.\n",
      "  Batch 6,000  of  8,916.    Elapsed: 0:23:07.\n",
      "  Batch 6,040  of  8,916.    Elapsed: 0:23:17.\n",
      "  Batch 6,080  of  8,916.    Elapsed: 0:23:26.\n",
      "  Batch 6,120  of  8,916.    Elapsed: 0:23:35.\n",
      "  Batch 6,160  of  8,916.    Elapsed: 0:23:44.\n",
      "  Batch 6,200  of  8,916.    Elapsed: 0:23:54.\n",
      "  Batch 6,240  of  8,916.    Elapsed: 0:24:03.\n",
      "  Batch 6,280  of  8,916.    Elapsed: 0:24:12.\n",
      "  Batch 6,320  of  8,916.    Elapsed: 0:24:22.\n",
      "  Batch 6,360  of  8,916.    Elapsed: 0:24:31.\n",
      "  Batch 6,400  of  8,916.    Elapsed: 0:24:40.\n",
      "  Batch 6,440  of  8,916.    Elapsed: 0:24:49.\n",
      "  Batch 6,480  of  8,916.    Elapsed: 0:24:59.\n",
      "  Batch 6,520  of  8,916.    Elapsed: 0:25:08.\n",
      "  Batch 6,560  of  8,916.    Elapsed: 0:25:17.\n",
      "  Batch 6,600  of  8,916.    Elapsed: 0:25:26.\n",
      "  Batch 6,640  of  8,916.    Elapsed: 0:25:36.\n",
      "  Batch 6,680  of  8,916.    Elapsed: 0:25:45.\n",
      "  Batch 6,720  of  8,916.    Elapsed: 0:25:54.\n",
      "  Batch 6,760  of  8,916.    Elapsed: 0:26:04.\n",
      "  Batch 6,800  of  8,916.    Elapsed: 0:26:13.\n",
      "  Batch 6,840  of  8,916.    Elapsed: 0:26:22.\n",
      "  Batch 6,880  of  8,916.    Elapsed: 0:26:31.\n",
      "  Batch 6,920  of  8,916.    Elapsed: 0:26:41.\n",
      "  Batch 6,960  of  8,916.    Elapsed: 0:26:50.\n",
      "  Batch 7,000  of  8,916.    Elapsed: 0:26:59.\n",
      "  Batch 7,040  of  8,916.    Elapsed: 0:27:08.\n",
      "  Batch 7,080  of  8,916.    Elapsed: 0:27:18.\n",
      "  Batch 7,120  of  8,916.    Elapsed: 0:27:27.\n",
      "  Batch 7,160  of  8,916.    Elapsed: 0:27:36.\n",
      "  Batch 7,200  of  8,916.    Elapsed: 0:27:45.\n",
      "  Batch 7,240  of  8,916.    Elapsed: 0:27:55.\n",
      "  Batch 7,280  of  8,916.    Elapsed: 0:28:04.\n",
      "  Batch 7,320  of  8,916.    Elapsed: 0:28:13.\n",
      "  Batch 7,360  of  8,916.    Elapsed: 0:28:23.\n",
      "  Batch 7,400  of  8,916.    Elapsed: 0:28:32.\n",
      "  Batch 7,440  of  8,916.    Elapsed: 0:28:41.\n",
      "  Batch 7,480  of  8,916.    Elapsed: 0:28:50.\n",
      "  Batch 7,520  of  8,916.    Elapsed: 0:29:00.\n",
      "  Batch 7,560  of  8,916.    Elapsed: 0:29:09.\n",
      "  Batch 7,600  of  8,916.    Elapsed: 0:29:18.\n",
      "  Batch 7,640  of  8,916.    Elapsed: 0:29:27.\n",
      "  Batch 7,680  of  8,916.    Elapsed: 0:29:37.\n",
      "  Batch 7,720  of  8,916.    Elapsed: 0:29:46.\n",
      "  Batch 7,760  of  8,916.    Elapsed: 0:29:55.\n",
      "  Batch 7,800  of  8,916.    Elapsed: 0:30:05.\n",
      "  Batch 7,840  of  8,916.    Elapsed: 0:30:14.\n",
      "  Batch 7,880  of  8,916.    Elapsed: 0:30:23.\n",
      "  Batch 7,920  of  8,916.    Elapsed: 0:30:32.\n",
      "  Batch 7,960  of  8,916.    Elapsed: 0:30:42.\n",
      "  Batch 8,000  of  8,916.    Elapsed: 0:30:51.\n",
      "  Batch 8,040  of  8,916.    Elapsed: 0:31:00.\n",
      "  Batch 8,080  of  8,916.    Elapsed: 0:31:09.\n",
      "  Batch 8,120  of  8,916.    Elapsed: 0:31:19.\n",
      "  Batch 8,160  of  8,916.    Elapsed: 0:31:28.\n",
      "  Batch 8,200  of  8,916.    Elapsed: 0:31:37.\n",
      "  Batch 8,240  of  8,916.    Elapsed: 0:31:47.\n",
      "  Batch 8,280  of  8,916.    Elapsed: 0:31:56.\n",
      "  Batch 8,320  of  8,916.    Elapsed: 0:32:05.\n",
      "  Batch 8,360  of  8,916.    Elapsed: 0:32:14.\n",
      "  Batch 8,400  of  8,916.    Elapsed: 0:32:24.\n",
      "  Batch 8,440  of  8,916.    Elapsed: 0:32:33.\n",
      "  Batch 8,480  of  8,916.    Elapsed: 0:32:42.\n",
      "  Batch 8,520  of  8,916.    Elapsed: 0:32:51.\n",
      "  Batch 8,560  of  8,916.    Elapsed: 0:33:01.\n",
      "  Batch 8,600  of  8,916.    Elapsed: 0:33:10.\n",
      "  Batch 8,640  of  8,916.    Elapsed: 0:33:19.\n",
      "  Batch 8,680  of  8,916.    Elapsed: 0:33:29.\n",
      "  Batch 8,720  of  8,916.    Elapsed: 0:33:38.\n",
      "  Batch 8,760  of  8,916.    Elapsed: 0:33:47.\n",
      "  Batch 8,800  of  8,916.    Elapsed: 0:33:56.\n",
      "  Batch 8,840  of  8,916.    Elapsed: 0:34:06.\n",
      "  Batch 8,880  of  8,916.    Elapsed: 0:34:15.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epcoh took: 0:34:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.36\n",
      "  Validation took: 0:03:55\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  8,916.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  8,916.    Elapsed: 0:00:19.\n",
      "  Batch   120  of  8,916.    Elapsed: 0:00:28.\n",
      "  Batch   160  of  8,916.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  8,916.    Elapsed: 0:00:46.\n",
      "  Batch   240  of  8,916.    Elapsed: 0:00:56.\n",
      "  Batch   280  of  8,916.    Elapsed: 0:01:05.\n",
      "  Batch   320  of  8,916.    Elapsed: 0:01:14.\n",
      "  Batch   360  of  8,916.    Elapsed: 0:01:23.\n",
      "  Batch   400  of  8,916.    Elapsed: 0:01:33.\n",
      "  Batch   440  of  8,916.    Elapsed: 0:01:42.\n",
      "  Batch   480  of  8,916.    Elapsed: 0:01:51.\n",
      "  Batch   520  of  8,916.    Elapsed: 0:02:01.\n",
      "  Batch   560  of  8,916.    Elapsed: 0:02:10.\n",
      "  Batch   600  of  8,916.    Elapsed: 0:02:19.\n",
      "  Batch   640  of  8,916.    Elapsed: 0:02:28.\n",
      "  Batch   680  of  8,916.    Elapsed: 0:02:38.\n",
      "  Batch   720  of  8,916.    Elapsed: 0:02:47.\n",
      "  Batch   760  of  8,916.    Elapsed: 0:02:56.\n",
      "  Batch   800  of  8,916.    Elapsed: 0:03:05.\n",
      "  Batch   840  of  8,916.    Elapsed: 0:03:15.\n",
      "  Batch   880  of  8,916.    Elapsed: 0:03:24.\n",
      "  Batch   920  of  8,916.    Elapsed: 0:03:33.\n",
      "  Batch   960  of  8,916.    Elapsed: 0:03:43.\n",
      "  Batch 1,000  of  8,916.    Elapsed: 0:03:52.\n",
      "  Batch 1,040  of  8,916.    Elapsed: 0:04:01.\n",
      "  Batch 1,080  of  8,916.    Elapsed: 0:04:10.\n",
      "  Batch 1,120  of  8,916.    Elapsed: 0:04:20.\n",
      "  Batch 1,160  of  8,916.    Elapsed: 0:04:29.\n",
      "  Batch 1,200  of  8,916.    Elapsed: 0:04:38.\n",
      "  Batch 1,240  of  8,916.    Elapsed: 0:04:47.\n",
      "  Batch 1,280  of  8,916.    Elapsed: 0:04:57.\n",
      "  Batch 1,320  of  8,916.    Elapsed: 0:05:06.\n",
      "  Batch 1,360  of  8,916.    Elapsed: 0:05:15.\n",
      "  Batch 1,400  of  8,916.    Elapsed: 0:05:24.\n",
      "  Batch 1,440  of  8,916.    Elapsed: 0:05:34.\n",
      "  Batch 1,480  of  8,916.    Elapsed: 0:05:43.\n",
      "  Batch 1,520  of  8,916.    Elapsed: 0:05:52.\n",
      "  Batch 1,560  of  8,916.    Elapsed: 0:06:02.\n",
      "  Batch 1,600  of  8,916.    Elapsed: 0:06:11.\n",
      "  Batch 1,640  of  8,916.    Elapsed: 0:06:20.\n",
      "  Batch 1,680  of  8,916.    Elapsed: 0:06:29.\n",
      "  Batch 1,720  of  8,916.    Elapsed: 0:06:39.\n",
      "  Batch 1,760  of  8,916.    Elapsed: 0:06:48.\n",
      "  Batch 1,800  of  8,916.    Elapsed: 0:06:57.\n",
      "  Batch 1,840  of  8,916.    Elapsed: 0:07:07.\n",
      "  Batch 1,880  of  8,916.    Elapsed: 0:07:16.\n",
      "  Batch 1,920  of  8,916.    Elapsed: 0:07:25.\n",
      "  Batch 1,960  of  8,916.    Elapsed: 0:07:34.\n",
      "  Batch 2,000  of  8,916.    Elapsed: 0:07:44.\n",
      "  Batch 2,040  of  8,916.    Elapsed: 0:07:53.\n",
      "  Batch 2,080  of  8,916.    Elapsed: 0:08:02.\n",
      "  Batch 2,120  of  8,916.    Elapsed: 0:08:11.\n",
      "  Batch 2,160  of  8,916.    Elapsed: 0:08:21.\n",
      "  Batch 2,200  of  8,916.    Elapsed: 0:08:30.\n",
      "  Batch 2,240  of  8,916.    Elapsed: 0:08:39.\n",
      "  Batch 2,280  of  8,916.    Elapsed: 0:08:48.\n",
      "  Batch 2,320  of  8,916.    Elapsed: 0:08:58.\n",
      "  Batch 2,360  of  8,916.    Elapsed: 0:09:07.\n",
      "  Batch 2,400  of  8,916.    Elapsed: 0:09:16.\n",
      "  Batch 2,440  of  8,916.    Elapsed: 0:09:26.\n",
      "  Batch 2,480  of  8,916.    Elapsed: 0:09:35.\n",
      "  Batch 2,520  of  8,916.    Elapsed: 0:09:44.\n",
      "  Batch 2,560  of  8,916.    Elapsed: 0:09:53.\n",
      "  Batch 2,600  of  8,916.    Elapsed: 0:10:03.\n",
      "  Batch 2,640  of  8,916.    Elapsed: 0:10:12.\n",
      "  Batch 2,680  of  8,916.    Elapsed: 0:10:21.\n",
      "  Batch 2,720  of  8,916.    Elapsed: 0:10:30.\n",
      "  Batch 2,760  of  8,916.    Elapsed: 0:10:40.\n",
      "  Batch 2,800  of  8,916.    Elapsed: 0:10:49.\n",
      "  Batch 2,840  of  8,916.    Elapsed: 0:10:58.\n",
      "  Batch 2,880  of  8,916.    Elapsed: 0:11:08.\n",
      "  Batch 2,920  of  8,916.    Elapsed: 0:11:17.\n",
      "  Batch 2,960  of  8,916.    Elapsed: 0:11:26.\n",
      "  Batch 3,000  of  8,916.    Elapsed: 0:11:35.\n",
      "  Batch 3,040  of  8,916.    Elapsed: 0:11:45.\n",
      "  Batch 3,080  of  8,916.    Elapsed: 0:11:54.\n",
      "  Batch 3,120  of  8,916.    Elapsed: 0:12:03.\n",
      "  Batch 3,160  of  8,916.    Elapsed: 0:12:12.\n",
      "  Batch 3,200  of  8,916.    Elapsed: 0:12:22.\n",
      "  Batch 3,240  of  8,916.    Elapsed: 0:12:31.\n",
      "  Batch 3,280  of  8,916.    Elapsed: 0:12:40.\n",
      "  Batch 3,320  of  8,916.    Elapsed: 0:12:50.\n",
      "  Batch 3,360  of  8,916.    Elapsed: 0:12:59.\n",
      "  Batch 3,400  of  8,916.    Elapsed: 0:13:08.\n",
      "  Batch 3,440  of  8,916.    Elapsed: 0:13:17.\n",
      "  Batch 3,480  of  8,916.    Elapsed: 0:13:27.\n",
      "  Batch 3,520  of  8,916.    Elapsed: 0:13:36.\n",
      "  Batch 3,560  of  8,916.    Elapsed: 0:13:45.\n",
      "  Batch 3,600  of  8,916.    Elapsed: 0:13:54.\n",
      "  Batch 3,640  of  8,916.    Elapsed: 0:14:04.\n",
      "  Batch 3,680  of  8,916.    Elapsed: 0:14:13.\n",
      "  Batch 3,720  of  8,916.    Elapsed: 0:14:22.\n",
      "  Batch 3,760  of  8,916.    Elapsed: 0:14:32.\n",
      "  Batch 3,800  of  8,916.    Elapsed: 0:14:41.\n",
      "  Batch 3,840  of  8,916.    Elapsed: 0:14:50.\n",
      "  Batch 3,880  of  8,916.    Elapsed: 0:14:59.\n",
      "  Batch 3,920  of  8,916.    Elapsed: 0:15:09.\n",
      "  Batch 3,960  of  8,916.    Elapsed: 0:15:18.\n",
      "  Batch 4,000  of  8,916.    Elapsed: 0:15:27.\n",
      "  Batch 4,040  of  8,916.    Elapsed: 0:15:36.\n",
      "  Batch 4,080  of  8,916.    Elapsed: 0:15:46.\n",
      "  Batch 4,120  of  8,916.    Elapsed: 0:15:55.\n",
      "  Batch 4,160  of  8,916.    Elapsed: 0:16:04.\n",
      "  Batch 4,200  of  8,916.    Elapsed: 0:16:14.\n",
      "  Batch 4,240  of  8,916.    Elapsed: 0:16:23.\n",
      "  Batch 4,280  of  8,916.    Elapsed: 0:16:32.\n",
      "  Batch 4,320  of  8,916.    Elapsed: 0:16:41.\n",
      "  Batch 4,360  of  8,916.    Elapsed: 0:16:51.\n",
      "  Batch 4,400  of  8,916.    Elapsed: 0:17:00.\n",
      "  Batch 4,440  of  8,916.    Elapsed: 0:17:09.\n",
      "  Batch 4,480  of  8,916.    Elapsed: 0:17:19.\n",
      "  Batch 4,520  of  8,916.    Elapsed: 0:17:28.\n",
      "  Batch 4,560  of  8,916.    Elapsed: 0:17:37.\n",
      "  Batch 4,600  of  8,916.    Elapsed: 0:17:46.\n",
      "  Batch 4,640  of  8,916.    Elapsed: 0:17:56.\n",
      "  Batch 4,680  of  8,916.    Elapsed: 0:18:05.\n",
      "  Batch 4,720  of  8,916.    Elapsed: 0:18:14.\n",
      "  Batch 4,760  of  8,916.    Elapsed: 0:18:23.\n",
      "  Batch 4,800  of  8,916.    Elapsed: 0:18:33.\n",
      "  Batch 4,840  of  8,916.    Elapsed: 0:18:42.\n",
      "  Batch 4,880  of  8,916.    Elapsed: 0:18:51.\n",
      "  Batch 4,920  of  8,916.    Elapsed: 0:19:01.\n",
      "  Batch 4,960  of  8,916.    Elapsed: 0:19:10.\n",
      "  Batch 5,000  of  8,916.    Elapsed: 0:19:19.\n",
      "  Batch 5,040  of  8,916.    Elapsed: 0:19:28.\n",
      "  Batch 5,080  of  8,916.    Elapsed: 0:19:38.\n",
      "  Batch 5,120  of  8,916.    Elapsed: 0:19:47.\n",
      "  Batch 5,160  of  8,916.    Elapsed: 0:19:56.\n",
      "  Batch 5,200  of  8,916.    Elapsed: 0:20:05.\n",
      "  Batch 5,240  of  8,916.    Elapsed: 0:20:15.\n",
      "  Batch 5,280  of  8,916.    Elapsed: 0:20:24.\n",
      "  Batch 5,320  of  8,916.    Elapsed: 0:20:33.\n",
      "  Batch 5,360  of  8,916.    Elapsed: 0:20:43.\n",
      "  Batch 5,400  of  8,916.    Elapsed: 0:20:52.\n",
      "  Batch 5,440  of  8,916.    Elapsed: 0:21:01.\n",
      "  Batch 5,480  of  8,916.    Elapsed: 0:21:10.\n",
      "  Batch 5,520  of  8,916.    Elapsed: 0:21:20.\n",
      "  Batch 5,560  of  8,916.    Elapsed: 0:21:29.\n",
      "  Batch 5,600  of  8,916.    Elapsed: 0:21:38.\n",
      "  Batch 5,640  of  8,916.    Elapsed: 0:21:48.\n",
      "  Batch 5,680  of  8,916.    Elapsed: 0:21:57.\n",
      "  Batch 5,720  of  8,916.    Elapsed: 0:22:06.\n",
      "  Batch 5,760  of  8,916.    Elapsed: 0:22:15.\n",
      "  Batch 5,800  of  8,916.    Elapsed: 0:22:25.\n",
      "  Batch 5,840  of  8,916.    Elapsed: 0:22:34.\n",
      "  Batch 5,880  of  8,916.    Elapsed: 0:22:43.\n",
      "  Batch 5,920  of  8,916.    Elapsed: 0:22:52.\n",
      "  Batch 5,960  of  8,916.    Elapsed: 0:23:02.\n",
      "  Batch 6,000  of  8,916.    Elapsed: 0:23:11.\n",
      "  Batch 6,040  of  8,916.    Elapsed: 0:23:20.\n",
      "  Batch 6,080  of  8,916.    Elapsed: 0:23:30.\n",
      "  Batch 6,120  of  8,916.    Elapsed: 0:23:39.\n",
      "  Batch 6,160  of  8,916.    Elapsed: 0:23:48.\n",
      "  Batch 6,200  of  8,916.    Elapsed: 0:23:57.\n",
      "  Batch 6,240  of  8,916.    Elapsed: 0:24:07.\n",
      "  Batch 6,280  of  8,916.    Elapsed: 0:24:16.\n",
      "  Batch 6,320  of  8,916.    Elapsed: 0:24:25.\n",
      "  Batch 6,360  of  8,916.    Elapsed: 0:24:34.\n",
      "  Batch 6,400  of  8,916.    Elapsed: 0:24:44.\n",
      "  Batch 6,440  of  8,916.    Elapsed: 0:24:53.\n",
      "  Batch 6,480  of  8,916.    Elapsed: 0:25:02.\n",
      "  Batch 6,520  of  8,916.    Elapsed: 0:25:12.\n",
      "  Batch 6,560  of  8,916.    Elapsed: 0:25:21.\n",
      "  Batch 6,600  of  8,916.    Elapsed: 0:25:30.\n",
      "  Batch 6,640  of  8,916.    Elapsed: 0:25:39.\n",
      "  Batch 6,680  of  8,916.    Elapsed: 0:25:49.\n",
      "  Batch 6,720  of  8,916.    Elapsed: 0:25:58.\n",
      "  Batch 6,760  of  8,916.    Elapsed: 0:26:07.\n",
      "  Batch 6,800  of  8,916.    Elapsed: 0:26:16.\n",
      "  Batch 6,840  of  8,916.    Elapsed: 0:26:26.\n",
      "  Batch 6,880  of  8,916.    Elapsed: 0:26:35.\n",
      "  Batch 6,920  of  8,916.    Elapsed: 0:26:44.\n",
      "  Batch 6,960  of  8,916.    Elapsed: 0:26:54.\n",
      "  Batch 7,000  of  8,916.    Elapsed: 0:27:03.\n",
      "  Batch 7,040  of  8,916.    Elapsed: 0:27:12.\n",
      "  Batch 7,080  of  8,916.    Elapsed: 0:27:21.\n",
      "  Batch 7,120  of  8,916.    Elapsed: 0:27:31.\n",
      "  Batch 7,160  of  8,916.    Elapsed: 0:27:40.\n",
      "  Batch 7,200  of  8,916.    Elapsed: 0:27:49.\n",
      "  Batch 7,240  of  8,916.    Elapsed: 0:27:58.\n",
      "  Batch 7,280  of  8,916.    Elapsed: 0:28:08.\n",
      "  Batch 7,320  of  8,916.    Elapsed: 0:28:17.\n",
      "  Batch 7,360  of  8,916.    Elapsed: 0:28:26.\n",
      "  Batch 7,400  of  8,916.    Elapsed: 0:28:36.\n",
      "  Batch 7,440  of  8,916.    Elapsed: 0:28:45.\n",
      "  Batch 7,480  of  8,916.    Elapsed: 0:28:54.\n",
      "  Batch 7,520  of  8,916.    Elapsed: 0:29:03.\n",
      "  Batch 7,560  of  8,916.    Elapsed: 0:29:13.\n",
      "  Batch 7,600  of  8,916.    Elapsed: 0:29:22.\n",
      "  Batch 7,640  of  8,916.    Elapsed: 0:29:31.\n",
      "  Batch 7,680  of  8,916.    Elapsed: 0:29:40.\n",
      "  Batch 7,720  of  8,916.    Elapsed: 0:29:50.\n",
      "  Batch 7,760  of  8,916.    Elapsed: 0:29:59.\n",
      "  Batch 7,800  of  8,916.    Elapsed: 0:30:08.\n",
      "  Batch 7,840  of  8,916.    Elapsed: 0:30:17.\n",
      "  Batch 7,880  of  8,916.    Elapsed: 0:30:27.\n",
      "  Batch 7,920  of  8,916.    Elapsed: 0:30:36.\n",
      "  Batch 7,960  of  8,916.    Elapsed: 0:30:45.\n",
      "  Batch 8,000  of  8,916.    Elapsed: 0:30:54.\n",
      "  Batch 8,040  of  8,916.    Elapsed: 0:31:04.\n",
      "  Batch 8,080  of  8,916.    Elapsed: 0:31:13.\n",
      "  Batch 8,120  of  8,916.    Elapsed: 0:31:22.\n",
      "  Batch 8,160  of  8,916.    Elapsed: 0:31:32.\n",
      "  Batch 8,200  of  8,916.    Elapsed: 0:31:41.\n",
      "  Batch 8,240  of  8,916.    Elapsed: 0:31:50.\n",
      "  Batch 8,280  of  8,916.    Elapsed: 0:31:59.\n",
      "  Batch 8,320  of  8,916.    Elapsed: 0:32:09.\n",
      "  Batch 8,360  of  8,916.    Elapsed: 0:32:18.\n",
      "  Batch 8,400  of  8,916.    Elapsed: 0:32:27.\n",
      "  Batch 8,440  of  8,916.    Elapsed: 0:32:36.\n",
      "  Batch 8,480  of  8,916.    Elapsed: 0:32:46.\n",
      "  Batch 8,520  of  8,916.    Elapsed: 0:32:55.\n",
      "  Batch 8,560  of  8,916.    Elapsed: 0:33:04.\n",
      "  Batch 8,600  of  8,916.    Elapsed: 0:33:13.\n",
      "  Batch 8,640  of  8,916.    Elapsed: 0:33:23.\n",
      "  Batch 8,680  of  8,916.    Elapsed: 0:33:32.\n",
      "  Batch 8,720  of  8,916.    Elapsed: 0:33:41.\n",
      "  Batch 8,760  of  8,916.    Elapsed: 0:33:50.\n",
      "  Batch 8,800  of  8,916.    Elapsed: 0:34:00.\n",
      "  Batch 8,840  of  8,916.    Elapsed: 0:34:09.\n",
      "  Batch 8,880  of  8,916.    Elapsed: 0:34:18.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epcoh took: 0:34:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.37\n",
      "  Validation took: 0:03:54\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  8,916.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  8,916.    Elapsed: 0:00:19.\n",
      "  Batch   120  of  8,916.    Elapsed: 0:00:28.\n",
      "  Batch   160  of  8,916.    Elapsed: 0:00:37.\n",
      "  Batch   200  of  8,916.    Elapsed: 0:00:46.\n",
      "  Batch   240  of  8,916.    Elapsed: 0:00:56.\n",
      "  Batch   280  of  8,916.    Elapsed: 0:01:05.\n",
      "  Batch   320  of  8,916.    Elapsed: 0:01:14.\n",
      "  Batch   360  of  8,916.    Elapsed: 0:01:23.\n",
      "  Batch   400  of  8,916.    Elapsed: 0:01:33.\n",
      "  Batch   440  of  8,916.    Elapsed: 0:01:42.\n",
      "  Batch   480  of  8,916.    Elapsed: 0:01:51.\n",
      "  Batch   520  of  8,916.    Elapsed: 0:02:01.\n",
      "  Batch   560  of  8,916.    Elapsed: 0:02:10.\n",
      "  Batch   600  of  8,916.    Elapsed: 0:02:19.\n",
      "  Batch   640  of  8,916.    Elapsed: 0:02:28.\n",
      "  Batch   680  of  8,916.    Elapsed: 0:02:38.\n",
      "  Batch   720  of  8,916.    Elapsed: 0:02:47.\n",
      "  Batch   760  of  8,916.    Elapsed: 0:02:56.\n",
      "  Batch   800  of  8,916.    Elapsed: 0:03:05.\n",
      "  Batch   840  of  8,916.    Elapsed: 0:03:15.\n",
      "  Batch   880  of  8,916.    Elapsed: 0:03:24.\n",
      "  Batch   920  of  8,916.    Elapsed: 0:03:33.\n",
      "  Batch   960  of  8,916.    Elapsed: 0:03:42.\n",
      "  Batch 1,000  of  8,916.    Elapsed: 0:03:52.\n",
      "  Batch 1,040  of  8,916.    Elapsed: 0:04:01.\n",
      "  Batch 1,080  of  8,916.    Elapsed: 0:04:10.\n",
      "  Batch 1,120  of  8,916.    Elapsed: 0:04:19.\n",
      "  Batch 1,160  of  8,916.    Elapsed: 0:04:29.\n",
      "  Batch 1,200  of  8,916.    Elapsed: 0:04:38.\n",
      "  Batch 1,240  of  8,916.    Elapsed: 0:04:47.\n",
      "  Batch 1,280  of  8,916.    Elapsed: 0:04:57.\n",
      "  Batch 1,320  of  8,916.    Elapsed: 0:05:06.\n",
      "  Batch 1,360  of  8,916.    Elapsed: 0:05:15.\n",
      "  Batch 1,400  of  8,916.    Elapsed: 0:05:24.\n",
      "  Batch 1,440  of  8,916.    Elapsed: 0:05:34.\n",
      "  Batch 1,480  of  8,916.    Elapsed: 0:05:43.\n",
      "  Batch 1,520  of  8,916.    Elapsed: 0:05:52.\n",
      "  Batch 1,560  of  8,916.    Elapsed: 0:06:01.\n",
      "  Batch 1,600  of  8,916.    Elapsed: 0:06:11.\n",
      "  Batch 1,640  of  8,916.    Elapsed: 0:06:20.\n",
      "  Batch 1,680  of  8,916.    Elapsed: 0:06:29.\n",
      "  Batch 1,720  of  8,916.    Elapsed: 0:06:38.\n",
      "  Batch 1,760  of  8,916.    Elapsed: 0:06:48.\n",
      "  Batch 1,800  of  8,916.    Elapsed: 0:06:57.\n",
      "  Batch 1,840  of  8,916.    Elapsed: 0:07:06.\n",
      "  Batch 1,880  of  8,916.    Elapsed: 0:07:16.\n",
      "  Batch 1,920  of  8,916.    Elapsed: 0:07:25.\n",
      "  Batch 1,960  of  8,916.    Elapsed: 0:07:34.\n",
      "  Batch 2,000  of  8,916.    Elapsed: 0:07:43.\n",
      "  Batch 2,040  of  8,916.    Elapsed: 0:07:53.\n",
      "  Batch 2,080  of  8,916.    Elapsed: 0:08:02.\n",
      "  Batch 2,120  of  8,916.    Elapsed: 0:08:11.\n",
      "  Batch 2,160  of  8,916.    Elapsed: 0:08:20.\n",
      "  Batch 2,200  of  8,916.    Elapsed: 0:08:30.\n",
      "  Batch 2,240  of  8,916.    Elapsed: 0:08:39.\n",
      "  Batch 2,280  of  8,916.    Elapsed: 0:08:48.\n",
      "  Batch 2,320  of  8,916.    Elapsed: 0:08:58.\n",
      "  Batch 2,360  of  8,916.    Elapsed: 0:09:07.\n",
      "  Batch 2,400  of  8,916.    Elapsed: 0:09:16.\n",
      "  Batch 2,440  of  8,916.    Elapsed: 0:09:25.\n",
      "  Batch 2,480  of  8,916.    Elapsed: 0:09:35.\n",
      "  Batch 2,520  of  8,916.    Elapsed: 0:09:44.\n",
      "  Batch 2,560  of  8,916.    Elapsed: 0:09:53.\n",
      "  Batch 2,600  of  8,916.    Elapsed: 0:10:02.\n",
      "  Batch 2,640  of  8,916.    Elapsed: 0:10:12.\n",
      "  Batch 2,680  of  8,916.    Elapsed: 0:10:21.\n",
      "  Batch 2,720  of  8,916.    Elapsed: 0:10:30.\n",
      "  Batch 2,760  of  8,916.    Elapsed: 0:10:40.\n",
      "  Batch 2,800  of  8,916.    Elapsed: 0:10:49.\n",
      "  Batch 2,840  of  8,916.    Elapsed: 0:10:58.\n",
      "  Batch 2,880  of  8,916.    Elapsed: 0:11:07.\n",
      "  Batch 2,920  of  8,916.    Elapsed: 0:11:17.\n",
      "  Batch 2,960  of  8,916.    Elapsed: 0:11:26.\n",
      "  Batch 3,000  of  8,916.    Elapsed: 0:11:35.\n",
      "  Batch 3,040  of  8,916.    Elapsed: 0:11:44.\n",
      "  Batch 3,080  of  8,916.    Elapsed: 0:11:54.\n",
      "  Batch 3,120  of  8,916.    Elapsed: 0:12:03.\n",
      "  Batch 3,160  of  8,916.    Elapsed: 0:12:12.\n",
      "  Batch 3,200  of  8,916.    Elapsed: 0:12:22.\n",
      "  Batch 3,240  of  8,916.    Elapsed: 0:12:31.\n",
      "  Batch 3,280  of  8,916.    Elapsed: 0:12:40.\n",
      "  Batch 3,320  of  8,916.    Elapsed: 0:12:49.\n",
      "  Batch 3,360  of  8,916.    Elapsed: 0:12:59.\n",
      "  Batch 3,400  of  8,916.    Elapsed: 0:13:08.\n",
      "  Batch 3,440  of  8,916.    Elapsed: 0:13:17.\n",
      "  Batch 3,480  of  8,916.    Elapsed: 0:13:26.\n",
      "  Batch 3,520  of  8,916.    Elapsed: 0:13:36.\n",
      "  Batch 3,560  of  8,916.    Elapsed: 0:13:45.\n",
      "  Batch 3,600  of  8,916.    Elapsed: 0:13:54.\n",
      "  Batch 3,640  of  8,916.    Elapsed: 0:14:03.\n",
      "  Batch 3,680  of  8,916.    Elapsed: 0:14:13.\n",
      "  Batch 3,720  of  8,916.    Elapsed: 0:14:22.\n",
      "  Batch 3,760  of  8,916.    Elapsed: 0:14:31.\n",
      "  Batch 3,800  of  8,916.    Elapsed: 0:14:41.\n",
      "  Batch 3,840  of  8,916.    Elapsed: 0:14:50.\n",
      "  Batch 3,880  of  8,916.    Elapsed: 0:14:59.\n",
      "  Batch 3,920  of  8,916.    Elapsed: 0:15:08.\n",
      "  Batch 3,960  of  8,916.    Elapsed: 0:15:18.\n",
      "  Batch 4,000  of  8,916.    Elapsed: 0:15:27.\n",
      "  Batch 4,040  of  8,916.    Elapsed: 0:15:36.\n",
      "  Batch 4,080  of  8,916.    Elapsed: 0:15:46.\n",
      "  Batch 4,120  of  8,916.    Elapsed: 0:15:55.\n",
      "  Batch 4,160  of  8,916.    Elapsed: 0:16:04.\n",
      "  Batch 4,200  of  8,916.    Elapsed: 0:16:13.\n",
      "  Batch 4,240  of  8,916.    Elapsed: 0:16:23.\n",
      "  Batch 4,280  of  8,916.    Elapsed: 0:16:32.\n",
      "  Batch 4,320  of  8,916.    Elapsed: 0:16:41.\n",
      "  Batch 4,360  of  8,916.    Elapsed: 0:16:50.\n",
      "  Batch 4,400  of  8,916.    Elapsed: 0:17:00.\n",
      "  Batch 4,440  of  8,916.    Elapsed: 0:17:09.\n",
      "  Batch 4,480  of  8,916.    Elapsed: 0:17:18.\n",
      "  Batch 4,520  of  8,916.    Elapsed: 0:17:28.\n",
      "  Batch 4,560  of  8,916.    Elapsed: 0:17:37.\n",
      "  Batch 4,600  of  8,916.    Elapsed: 0:17:46.\n",
      "  Batch 4,640  of  8,916.    Elapsed: 0:17:55.\n",
      "  Batch 4,680  of  8,916.    Elapsed: 0:18:05.\n",
      "  Batch 4,720  of  8,916.    Elapsed: 0:18:14.\n",
      "  Batch 4,760  of  8,916.    Elapsed: 0:18:23.\n",
      "  Batch 4,800  of  8,916.    Elapsed: 0:18:33.\n",
      "  Batch 4,840  of  8,916.    Elapsed: 0:18:42.\n",
      "  Batch 4,880  of  8,916.    Elapsed: 0:18:51.\n",
      "  Batch 4,920  of  8,916.    Elapsed: 0:19:00.\n",
      "  Batch 4,960  of  8,916.    Elapsed: 0:19:10.\n",
      "  Batch 5,000  of  8,916.    Elapsed: 0:19:19.\n",
      "  Batch 5,040  of  8,916.    Elapsed: 0:19:28.\n",
      "  Batch 5,080  of  8,916.    Elapsed: 0:19:37.\n",
      "  Batch 5,120  of  8,916.    Elapsed: 0:19:47.\n",
      "  Batch 5,160  of  8,916.    Elapsed: 0:19:56.\n",
      "  Batch 5,200  of  8,916.    Elapsed: 0:20:05.\n",
      "  Batch 5,240  of  8,916.    Elapsed: 0:20:15.\n",
      "  Batch 5,280  of  8,916.    Elapsed: 0:20:24.\n",
      "  Batch 5,320  of  8,916.    Elapsed: 0:20:33.\n",
      "  Batch 5,360  of  8,916.    Elapsed: 0:20:42.\n",
      "  Batch 5,400  of  8,916.    Elapsed: 0:20:52.\n",
      "  Batch 5,440  of  8,916.    Elapsed: 0:21:01.\n",
      "  Batch 5,480  of  8,916.    Elapsed: 0:21:10.\n",
      "  Batch 5,520  of  8,916.    Elapsed: 0:21:19.\n",
      "  Batch 5,560  of  8,916.    Elapsed: 0:21:29.\n",
      "  Batch 5,600  of  8,916.    Elapsed: 0:21:38.\n",
      "  Batch 5,640  of  8,916.    Elapsed: 0:21:47.\n",
      "  Batch 5,680  of  8,916.    Elapsed: 0:21:57.\n",
      "  Batch 5,720  of  8,916.    Elapsed: 0:22:06.\n",
      "  Batch 5,760  of  8,916.    Elapsed: 0:22:15.\n",
      "  Batch 5,800  of  8,916.    Elapsed: 0:22:24.\n",
      "  Batch 5,840  of  8,916.    Elapsed: 0:22:34.\n",
      "  Batch 5,880  of  8,916.    Elapsed: 0:22:43.\n",
      "  Batch 5,920  of  8,916.    Elapsed: 0:22:52.\n",
      "  Batch 5,960  of  8,916.    Elapsed: 0:23:01.\n",
      "  Batch 6,000  of  8,916.    Elapsed: 0:23:11.\n",
      "  Batch 6,040  of  8,916.    Elapsed: 0:23:20.\n",
      "  Batch 6,080  of  8,916.    Elapsed: 0:23:29.\n",
      "  Batch 6,120  of  8,916.    Elapsed: 0:23:39.\n",
      "  Batch 6,160  of  8,916.    Elapsed: 0:23:48.\n",
      "  Batch 6,200  of  8,916.    Elapsed: 0:23:57.\n",
      "  Batch 6,240  of  8,916.    Elapsed: 0:24:06.\n",
      "  Batch 6,280  of  8,916.    Elapsed: 0:24:16.\n",
      "  Batch 6,320  of  8,916.    Elapsed: 0:24:25.\n",
      "  Batch 6,360  of  8,916.    Elapsed: 0:24:34.\n",
      "  Batch 6,400  of  8,916.    Elapsed: 0:24:44.\n",
      "  Batch 6,440  of  8,916.    Elapsed: 0:24:53.\n",
      "  Batch 6,480  of  8,916.    Elapsed: 0:25:02.\n",
      "  Batch 6,520  of  8,916.    Elapsed: 0:25:11.\n",
      "  Batch 6,560  of  8,916.    Elapsed: 0:25:21.\n",
      "  Batch 6,600  of  8,916.    Elapsed: 0:25:30.\n",
      "  Batch 6,640  of  8,916.    Elapsed: 0:25:39.\n",
      "  Batch 6,680  of  8,916.    Elapsed: 0:25:48.\n",
      "  Batch 6,720  of  8,916.    Elapsed: 0:25:58.\n",
      "  Batch 6,760  of  8,916.    Elapsed: 0:26:07.\n",
      "  Batch 6,800  of  8,916.    Elapsed: 0:26:16.\n",
      "  Batch 6,840  of  8,916.    Elapsed: 0:26:26.\n",
      "  Batch 6,880  of  8,916.    Elapsed: 0:26:35.\n",
      "  Batch 6,920  of  8,916.    Elapsed: 0:26:44.\n",
      "  Batch 6,960  of  8,916.    Elapsed: 0:26:53.\n",
      "  Batch 7,000  of  8,916.    Elapsed: 0:27:03.\n",
      "  Batch 7,040  of  8,916.    Elapsed: 0:27:12.\n",
      "  Batch 7,080  of  8,916.    Elapsed: 0:27:21.\n",
      "  Batch 7,120  of  8,916.    Elapsed: 0:27:31.\n",
      "  Batch 7,160  of  8,916.    Elapsed: 0:27:40.\n",
      "  Batch 7,200  of  8,916.    Elapsed: 0:27:49.\n",
      "  Batch 7,240  of  8,916.    Elapsed: 0:27:58.\n",
      "  Batch 7,280  of  8,916.    Elapsed: 0:28:08.\n",
      "  Batch 7,320  of  8,916.    Elapsed: 0:28:17.\n",
      "  Batch 7,360  of  8,916.    Elapsed: 0:28:26.\n",
      "  Batch 7,400  of  8,916.    Elapsed: 0:28:35.\n",
      "  Batch 7,440  of  8,916.    Elapsed: 0:28:45.\n",
      "  Batch 7,480  of  8,916.    Elapsed: 0:28:54.\n",
      "  Batch 7,520  of  8,916.    Elapsed: 0:29:03.\n",
      "  Batch 7,560  of  8,916.    Elapsed: 0:29:12.\n",
      "  Batch 7,600  of  8,916.    Elapsed: 0:29:22.\n",
      "  Batch 7,640  of  8,916.    Elapsed: 0:29:31.\n",
      "  Batch 7,680  of  8,916.    Elapsed: 0:29:40.\n",
      "  Batch 7,720  of  8,916.    Elapsed: 0:29:50.\n",
      "  Batch 7,760  of  8,916.    Elapsed: 0:29:59.\n",
      "  Batch 7,800  of  8,916.    Elapsed: 0:30:08.\n",
      "  Batch 7,840  of  8,916.    Elapsed: 0:30:17.\n",
      "  Batch 7,880  of  8,916.    Elapsed: 0:30:27.\n",
      "  Batch 7,920  of  8,916.    Elapsed: 0:30:36.\n",
      "  Batch 7,960  of  8,916.    Elapsed: 0:30:45.\n",
      "  Batch 8,000  of  8,916.    Elapsed: 0:30:55.\n",
      "  Batch 8,040  of  8,916.    Elapsed: 0:31:04.\n",
      "  Batch 8,080  of  8,916.    Elapsed: 0:31:13.\n",
      "  Batch 8,120  of  8,916.    Elapsed: 0:31:22.\n",
      "  Batch 8,160  of  8,916.    Elapsed: 0:31:32.\n",
      "  Batch 8,200  of  8,916.    Elapsed: 0:31:41.\n",
      "  Batch 8,240  of  8,916.    Elapsed: 0:31:50.\n",
      "  Batch 8,280  of  8,916.    Elapsed: 0:31:59.\n",
      "  Batch 8,320  of  8,916.    Elapsed: 0:32:09.\n",
      "  Batch 8,360  of  8,916.    Elapsed: 0:32:18.\n",
      "  Batch 8,400  of  8,916.    Elapsed: 0:32:27.\n",
      "  Batch 8,440  of  8,916.    Elapsed: 0:32:37.\n",
      "  Batch 8,480  of  8,916.    Elapsed: 0:32:46.\n",
      "  Batch 8,520  of  8,916.    Elapsed: 0:32:55.\n",
      "  Batch 8,560  of  8,916.    Elapsed: 0:33:04.\n",
      "  Batch 8,600  of  8,916.    Elapsed: 0:33:14.\n",
      "  Batch 8,640  of  8,916.    Elapsed: 0:33:23.\n",
      "  Batch 8,680  of  8,916.    Elapsed: 0:33:32.\n",
      "  Batch 8,720  of  8,916.    Elapsed: 0:33:41.\n",
      "  Batch 8,760  of  8,916.    Elapsed: 0:33:51.\n",
      "  Batch 8,800  of  8,916.    Elapsed: 0:34:00.\n",
      "  Batch 8,840  of  8,916.    Elapsed: 0:34:09.\n",
      "  Batch 8,880  of  8,916.    Elapsed: 0:34:19.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:34:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.45\n",
      "  Validation took: 0:03:55\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:55:01 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        m = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "#                              attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        loss, logits = m['loss'], m['logits']\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            m = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "#                                    attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "            loss, logits = m['loss'], m['logits']\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "J3xKUh9Ly_IJ",
    "outputId": "3db5cc50-4290-4162-ea87-c01ae864a59d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0:34:23</td>\n",
       "      <td>0:03:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0:34:27</td>\n",
       "      <td>0:03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0:34:27</td>\n",
       "      <td>0:03:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.45         0.36           0.87       0:34:23         0:03:55\n",
       "2               0.26         0.37           0.87       0:34:27         0:03:54\n",
       "3               0.15         0.45           0.87       0:34:27         0:03:55"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "aRwNZgjoy_5W",
    "outputId": "a00ee51d-a239-438e-df1f-f93287c2addc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAH1CAYAAAA9J4XUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXiU9b3//9c9M8lkT2YmC2EnQSFCOMUNW49aoQoqFBVbz6Eu9VQuaxd+XrWttLYKtdWTb/2entbW2l2pp/Vqiy01demltW5fxY3TUDYxCaASEjKTBbJNZub+/THJMJOFJJDMfSd5Pq5LSWY+M/dnwvDO/ZrPchumaZoCAAAAgEnOYXUHAAAAAMAOCEcAAAAAIMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHAEAAACAJMIRAExKN998s/74xz+Oels7e//99zVv3jyFQiFJJ35dfduO1EMPPaQ777zzpPsKALCGwXWOAGB8WLx4cezrjo4Opaamyul0SpI2bdqkj3/841Z17aQ1Nzdrw4YNeuONN5Senq4bb7xR69atG7T9ihUrdPPNN+uaa65JuP2RRx7R1q1b9fjjjw/62Pfff1/Lli3Tzp075XK5TtivkbTdtm2bvvKVr+jFF188YbvR8Pjjj+v3v/+9fvvb3475sQBgMjpxxQcA2Mb27dtjXy9dulTf/va39ZGPfKRfu1AoNOQJvV384he/UFdXl15++WUFg0G9++67J2x/1VVXaevWrf3C0datW3XVVVeNZVcBAJMA0+oAYJzbtm2bLrzwQv30pz/V+eefr6997WtqaWnRLbfcovPOO0/nnHOObrnlFh0+fDj2mOuvv16///3vJUVHI/793/9dFRUVOuecc7R06VK98MILJ9X2vffe06c+9SktXrxYn/70p7Vp0yZ9+ctfHrTvLpdLXq9X6enpys3N1VlnnXXC17p69Wq99dZb+uCDD2K3vfvuu3rnnXd0xRVX6O9//7uuvPJKnXnmmbrooov0wAMPDPpc8a8rHA6roqJCS5Ys0bJlyxJekyRt2bJFl112mRYvXqxly5bpsccekyS1t7dr3bp1amho0OLFi7V48WLV19frgQceSHjdzz33nK644gqdffbZuv7661VdXR27b+nSpfrFL36hVatW6ayzztJtt92mrq6uE/4cBvL2229rzZo1Ouuss7RmzRq9/fbbsfsef/xxLVu2TIsXL9bSpUv15z//WZJ04MABXXfddTrrrLO0ZMkS3XbbbSM+LgBMJIQjAJgAGhsb1dLSoueff1733HOPIpGIrr76aj3//PN6/vnn5Xa79a1vfWvQx1dVVWnOnDl67bXXdPPNN+vOO+/UYLOuT9T2y1/+shYtWqRt27bpC1/4grZu3XrCfpeXl+svf/lLLKQMZcqUKVqyZEnC827dulUXXnhhLGRVVFTozTff1E9+8hP99re/1bPPPjvk8/7ud7/T888/rz/96U/asmWLnn766YT7fT6ffvKTn+jtt9/Wfffdp/vuu087d+5URkaGfvazn6mwsFDbt2/X9u3bVVRUlPDY2tpa3X777fr617+uV199VRdeeKE++9nPKhgMxto89dRT+vnPf67nnntOe/fuPeH0wIE0Nzfrlltu0fXXX69t27bppptu0i233KKmpia1t7fr29/+tn72s59p+/bteuyxx1RWViZJ+v73v6/zzz9fb7zxhl588UVdd911IzouAEw0hCMAmAAcDofWr1+v1NRUpaWlyePxaPny5UpPT1dWVpZuvfVWvfHGG4M+furUqfrkJz8pp9Opq666SkeOHFFjY+OI2h46dEg7duyI9ePss8/W0qVLBz3mgQMHdNddd+nXv/61fvazn+kPf/iDJCkYDGrhwoU6evTogI+78sorY+EoEonoiSeeiE2pW7JkiebNmyeHw6H58+friiuu0Ouvvz7kz++pp57SjTfeqOLiYuXl5emWW25JuP+jH/2oZs6cKcMwdO655+r888/Xm2++OeTzStKTTz6piy66SOeff75SUlL0mc98Rp2dnQnTJK+//noVFRUpLy9PF198sXbv3j2s5+7197//XbNmzdKVV14pl8ullStXqqSkRM8//7yk6Ptj37596uzsVGFhoU477TRJ0ZG7Q4cOqaGhQW63W2efffaIjgsAE834mJQOADghj8cjt9sd+76jo0P33XefXnrpJbW0tEiS2traFA6HY5s4xMvPz499nZ6eLik6ZWwgg7VtampSbm5u7DZJKi4uVl1d3YDP84c//EFLly7VOeeco1/84hf61Kc+JUmaOXOm5s2bp+zs7AEfd+mll2rTpk363//9X3V0dKijo0MXXXSRJOkf//iH7r//fu3bt0/d3d0KBoNasWLFgM8Tr6GhQcXFxbHvp06dmnD/Cy+8oB/96Efav3+/IpGIOjs7dfrppw/5vL3PHf98DodDxcXFqq+vj91WUFAQ+zo9PV0NDQ3Deu7BjtH7Gurr65WRkaHvfe97+uUvf6k777xTZ555pu644w6VlpbqK1/5ir7//e/rmmuuUW5urm666aZ+67kAYDIhHAHABGAYRsL3v/zlL1VbW6vf/e53Kigo0O7du3XllVcOOlVuNBQUFKilpUUdHR2xgDRYMJKiG0d0d3dLkmbMmKGf//znuuGGG5STk6MvfelLgz4uPT1dy5cv15/+9Cd1dXXpiiuuUGpqqiTp9ttv13XXXaef//zncrvd+s53vqOmpqZh9T2+r/FfB4NBrV+/XhUVFVq2bJlSUlL0uc99Lvaz7Puz76uwsFDvvPNO7HvTNFVXV9dv+t2pKCws1KFDhxJuq6ur0wUXXCBJuuCCC3TBBReos7NT//3f/61vfvOb+s1vfqOCggJ9+9vfliS9+eabuummm3TOOedo1qxZo9Y3ABhPmFYHABNQW1ub3G63cnJy1NzcrB/+8Idjfsxp06Zp4cKFeuCBBxQMBrV9+/bYtK6BXHrppXrqqaf07LPPKhwOKysrS/Pnz9fBgwcTRp8GctVVV+mpp57SM888oyuvvDJ2e1tbm3Jzc+V2u1VVVaXKysph9f2yyy7Tr3/9ax0+fFgtLS366U9/GrsvGAwqGAzK6/XK5XLphRde0CuvvBK73+fzqbm5edBpgJdddpleeOEFvfrqq+ru7tYvf/lLpaamJmzNPhKmaaqrqyvhv4suukj79+/XE088oVAopCeffFLvvvuuPvrRj6qxsVHPPvus2tvblZqaqoyMDDkc0V//Tz31VGyjjtzcXBmGEbsPACYjRo4AYAK68cYb9eUvf1nnnXeeCgsLddNNNw1rY4JTdf/992vDhg1asmSJFi1apMsvv1zhcHjAtosXL9b999+vH/7wh/rKV74ij8eja665RmvXrtWXvvQl/frXv9YZZ5wx4GPPOeccZWVlye12a9GiRbHb7777blVUVOhb3/qWzj33XF122WVqbW0dst+f/OQntX//fq1evVqZmZn6zGc+o9dee02SlJWVpW984xu67bbbFAwGdfHFFyespSotLdUVV1yhj33sYwqHw/rLX/6S8NwlJSX67ne/q3vuuUf19fUqKyvTQw89FBvtGqnt27cnvGZJ2rlzpx566CHde++92rhxo2bNmqWHHnpIXq9XDQ0Nevjhh3XHHXfIMAyVlZVp48aNkqQdO3bo3nvv1bFjx+Tz+XTnnXdqxowZJ9UvAJgIuAgsAGDM3HbbbSopKdH69eut7goAAENi7BwAMGqqqqp08OBBRSIRvfjii3ruuef0sY99zOpuAQAwLEyrAwCMmsbGRn3xi19Uc3OzpkyZoo0bNw46NQ4AALthWh0AAAAAiGl1AAAAACCJcAQAAAAAkibgmqOmpjZFIvaYKejzZcnvP2Z1NwBgUqMWA4D17FKLHQ5DHk/moPdPuHAUiZi2CUeSbNUXAJisqMUAYL3xUIuTNq2utrZW1157rZYvX65rr71W+/fv79fmyJEjuvXWW7Vq1Spddtll2rp1a7K6BwAAAGCSS1o4uvvuu7V27Vo988wzWrt2re66665+bf7zP/9TCxcu1BNPPKH/+Z//0fe+9z3V1dUlq4sAAAAAJrGkhCO/369du3Zp5cqVkqSVK1dq165dCgQCCe327NmjCy64QJLk9Xo1f/58PfXUU8noIgAAAIBJLilrjurq6lRUVCSn0ylJcjqdKiwsVF1dnbxeb6zdggUL9OSTT6q8vFzvv/++tm/frunTp4/oWD5f1qj2/VQVFGRb3QUAmPSoxQBgvfFQi221IcOGDRt07733avXq1Zo6dao+/OEPxwLVcPn9x2yz2KugIFtHjhy1uhsAMKlRiwHAenapxQ6HccLBlKSEo+LiYtXX1yscDsvpdCocDquhoUHFxcUJ7bxer+6///7Y9+vWrdPcuXOT0UUAAAAAk1xS1hz5fD6VlZWpsrJSklRZWamysrKEKXWS1NTUpFAoJEl69dVX9c4778TWKQEAAADAWEratLqNGzdqw4YNevDBB5WTk6OKigpJ0dGh9evXq7y8XFVVVfrOd74jh8Mhj8ejhx56SOnp6cnqIgAAAIBJzDBN0x4LdEYJa44AAPGoxQBgPbvU4qHWHCXtOkcAAAAAYGeEIwAAAAAQ4QgAAAAAJBGOAAAAAEAS4QgAAAAAJCVxK28AAJKp9bX/p8bHt+idpoBcHq/yr16jnPM+YnW3AGBSGW+1mHAEAJhwWl/7f6rf/LDMYFCSFAr4Vb/5YUmy9S9lAJhIxmMtJhwBABKYpimZphSJyDQjUsSUGYlIPf+ZvfdFIpIZGfy+3sf1PocZibuv9/GDPJdpxrVL7MvxrwdrH1HL3/8W+2Uce13BoOof3azO/bUj+GkYJ/+DNE7+sSf9yFM45qm8VKt+TlYc07Civ9EDn8Jjk/5Aa/5edQp/P+Px79WC44705xt46skBa3Hj41sIRwBwqhJO2iODnBybkcSTanOgE+y+9yWerB9/zriTfLM3AMSdhJuJgWDQ5zL7Ps7s0/eBjtP/tn597hMIjj+XmfCcA7aPP06//tjjQtonxeGQ4XDIDIUGvNvs7FTrKy+PfT9O4Wd48g89hb83q/7OT+W4FvTZ5Odk32NiXAkF/FZ3YVCEozEw3uZW4tQN52S39+R8oE/eT3yy2/ek3+x/X99P3gcMEGbC/YN+eh9/4tyvPyczEmAmnrT3O078ifwgIwG94WO8/sI1jOhJe++fDsfx2xyOuNscksOQYTj63Gf03Hf869h9LlficzgcPY+Pb2fE7pNh9Gl3ov4Yif1yDNI+vj89tx9/LiOxzYmOc8LnMhJe2/F+9Wnfo+artw/4y9fl9ank//zfZP7tA0iSkw6vhN5hHnLkx9x/5x0KBQL9bnd5faPRpTFBOBpldptbOfhJ6AlOjvueqPadJjPoiXOf9n1PnBM+XTeHPDlOaN/nk/P+n4j3vW+wkYATfHof159+U3YiZr/jJNw/XvU5WY+doCacOCeejCee7BoDn6A6HHK4XHEnuwOc2PY5kR/0ZDfuJDv+BLtvIEhon/A8A4SEwYLGCQNEn4AyZKBxJIYiJFX+1WsSarEkGampyr96jYW9AjCWxt2UunHmZH5K+VdfM+5qsWFaNkY8Nvz+Y4pErHtJg31aaaSkKmPBgj5TYwY4CR/WSMAAJ/uDBJpx64SfTg/8iXj/T8tPdLJr9H/+U/j0fqCT9Vh/+p5gD/HJ+wmfa8DX3eeT+gECRMKn+EbccfiFgAmsdxQ/xCg+AFjGbrXY4TDk82UNej/haJS9c/OnB70vdfqMfifaA56gD+tkt//Jceykd4CRgIFP+vsep/+n5f0/ve/7ifjwp9v0P+k3BumXI3l/YQAmvIKCbB05ctTqbgDApGaXWjxUOGJa3ShzeX2DznOfvfEeC3oEAAAAYDj4iH6U5V+9RkZqasJtdp9bCQAAAICRo1HXO4fSTnMrAQAAAAyNcDQGcs77iHLO+4ht5lYCAAAAGBrT6gAAAABAhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkCS5knWg2tpabdiwQc3NzcrLy1NFRYVmz56d0Mbv9+trX/ua6urqFAqFtGTJEn3jG9+Qy5W0bgIAAACYpJI2cnT33Xdr7dq1euaZZ7R27Vrddddd/do89NBDKi0t1RNPPKE///nP2rlzp/76178mq4sAAAAAJrGkhCO/369du3Zp5cqVkqSVK1dq165dCgQCCe0Mw1BbW5sikYiCwaC6u7tVVFSUjC4CAAAAmOSSEo7q6upUVFQkp9MpSXI6nSosLFRdXV1Cu8997nOqra3Vv/7rv8b+O+uss5LRRQAAAACTnK0W8zz99NOaN2+eHnnkEbW1tWndunV6+umntWLFimE/h8+XNYY9HLmCgmyruwAAkx61GACsNx5qcVLCUXFxserr6xUOh+V0OhUOh9XQ0KDi4uKEdo8++qjuvfdeORwOZWdna+nSpdq2bduIwpHff0yRiDnaL+GkFBRk68iRo1Z3AwAmNWoxAFjPLrXY4TBOOJiSlGl1Pp9PZWVlqqyslCRVVlaqrKxMXq83od306dP14osvSpKCwaBeffVVnXbaacnoIgAAAIBJzjBNMynDLNXV1dqwYYNaW1uVk5OjiooKlZSUaN26dVq/fr3Ky8t18OBB3X333WpsbFQ4HNaSJUt05513jmgrb0aOAADxqMUAYD271OKhRo6SFo6ShXAEAIhHLQYA69mlFttiWh0AAAAA2B3hCAAAAABEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJAkuZJ1oNraWm3YsEHNzc3Ky8tTRUWFZs+endDmq1/9qvbu3Rv7fu/evfrRj36kZcuWJaubAAAAACYpwzRNMxkHuuGGG7RmzRqtXr1aW7du1ZYtW7R58+ZB2+/Zs0c33nijXnrpJaWmpg77OH7/MUUiSXlJQyooyNaRI0et7gYATGrUYgCwnl1qscNhyOfLGvz+ZHTC7/dr165dWrlypSRp5cqV2rVrlwKBwKCP+cMf/qBVq1aNKBgBAAAAwMlKyrS6uro6FRUVyel0SpKcTqcKCwtVV1cnr9fbr30wGNQTTzyhhx9+eMTHOlEStEJBQbbVXQCASY9aDADWGw+1OGlrjkbi2Wef1dSpU1VWVjbixzKtDgAQj1oMANazSy22xbS64uJi1dfXKxwOS5LC4bAaGhpUXFw8YPstW7ZozZo1yegaAAAAAEhKUjjy+XwqKytTZWWlJKmyslJlZWUDTqk7fPiw3nrrLa1atSoZXQMAAAAASUm8ztHGjRv16KOPavny5Xr00Ue1adMmSdK6deu0Y8eOWLs//vGPuvjii5Wbm5usrgEAAABA8rbyThbWHAEA4lGLAcB6dqnFtlhzBAAAAAB2RzgCAAAAABGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEBSEsNRbW2trr32Wi1fvlzXXnut9u/fP2C7J598UqtWrdLKlSu1atUqNTY2JquLAAAAACYxV7IOdPfdd2vt2rVavXq1tm7dqrvuukubN29OaLNjxw798Ic/1COPPKKCggIdPXpUqampyeoiAAAAgEksKSNHfr9fu3bt0sqVKyVJK1eu1K5duxQIBBLaPfzww/qP//gPFRQUSJKys7PldruT0UUAAAAAk1xSRo7q6upUVFQkp9MpSXI6nSosLFRdXZ28Xm+sXXV1taZPn65PfepTam9v1yWXXKJbb71VhmEM+1g+X9ao9/9UFBRkW90FAJj0qMUAYL3xUIuTNq1uOMLhsPbu3atf/epXCgaDuvnmmzV16lRdeeWVw34Ov/+YIhFzDHs5fAUF2Tpy5KjV3QCASY1aDADWs0stdjiMEw6mJGVaXXFxserr6xUOhyVFQ1BDQ4OKi4sT2k2dOlUrVqxQamqqsrKytGzZMlVVVSWjiwAAAAAmuaSEI5/Pp7KyMlVWVkqSKisrVVZWljClToquRXr55Zdlmqa6u7v12muvaf78+cnoIgAAAIBJLmlbeW/cuFGPPvqoli9frkcffVSbNm2SJK1bt047duyQJF1xxRXy+Xy6/PLLdeWVV2ru3Lm65pprktVFAAAAAJOYYZqmPRbojBLWHAEA4lGLAcB6dqnFtlhzBAAAAAB2RzgCAAAAABGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJI0gHL322mt67733JEkNDQ2644479LWvfU1HjhwZs84BAAAAQLIMOxxt2rRJTqdTklRRUaFQKCTDMPTNb35zzDoHAAAAAMniGm7D+vp6TZ06VaFQSC+//LL+9re/KSUlRRdccMFY9g8AAAAAkmLY4SgrK0uNjY3at2+fSktLlZmZqWAwqFAoNJb9AwAAAICkGHY4uu6663TNNdeou7tbX//61yVJb7/9tkpKSsascwAAAACQLIZpmuZwG9fW1srpdGrmzJmx74PBoObNmzdmHRwpv/+YIpFhv6QxVVCQrSNHjlrdDQCY1KjFAGA9u9Rih8OQz5c16P3DHjmSpDlz5sS+fu211+RwOHTuueeefO8AAAAAwCaGvVvdddddp7feekuS9NOf/lRf+tKXdPvtt+uhhx4as84BAAAAQLIMOxzt27dPH/rQhyRJv//977V582b97ne/02OPPTZmnQMAAACAZBn2tLpIJCLDMHTw4EGZpqm5c+dKklpaWsascwAAAACQLMMOR2eddZa+9a1v6ciRI7rkkkskSQcPHpTH4xmzzgEAAABAsgx7Wt19992nnJwczZs3T1/4whckSTU1NbrhhhuG9fja2lpde+21Wr58ua699lrt37+/X5sHHnhAH/7wh7V69WqtXr1amzZtGm73AAAAAOCUjGgr71Nxww03aM2aNVq9erW2bt2qLVu2aPPmzQltHnjgAbW3t+uOO+446eOwlTcAIB61GACsZ5daPNRW3sMeOeru7tYPfvADLVu2TOXl5Vq2bJl+8IMfKBgMDvlYv9+vXbt2aeXKlZKklStXateuXQoEAsM9PAAAAACMqWGvOfrud7+rqqoqbdq0SVOnTtWhQ4f04IMP6tixY/r6179+wsfW1dWpqKhITqdTkuR0OlVYWKi6ujp5vd6Etn/5y1/08ssvq6CgQF/84he1ePHiEb2gEyVBKxQUZFvdBQCY9KjFAGC98VCLhx2Onn76aW3dujW2AUNJSYnOOOMMrV69eshwNFz/9m//ps9+9rNKSUnRK6+8os997nN68sknR7TpA9PqAADxqMUAYD271OJRm1Y32NKk4SxZKi4uVn19vcLhsCQpHA6roaFBxcXFCe0KCgqUkpIiSTr//PNVXFysffv2DbeLAAAAAHDShh2OVqxYoVtvvVUvvfSSqqur9eKLL+rzn/+8LrvssiEf6/P5VFZWpsrKSklSZWWlysrK+k2pq6+vj329e/duffDBB5ozZ85wuwgAAAAAJ23Yu9UFg0H9+Mc/VmVlpRoaGlRUVKTLL79cwWBQX/3qV4d8fHV1tTZs2KDW1lbl5OSooqJCJSUlWrdundavX6/y8nLdcccd2rlzpxwOh1JSUrR+/XpddNFFI3pBTKsDAMSjFgOA9exSi4eaVndKW3l3dXXpQx/6kHbv3n2yTzHqCEcAgHjUYgCwnl1q8aitORqIYRjDWnMEAAAAAHZ3SuFIigYkAAAAABjvhtzK+9VXXx30vu7u7lHtDAAAAABYZchwdOedd57w/r7bcQMAAADAeDRkOPrb3/6WjH4AAAAAgKVOec0RAAAAAEwEhCMAAAAAEOEIAAAAACQRjgAAAABAEuEIAAAAACFhbQcAACAASURBVCQRjgAAAABAEuEIAAAAACQRjgAAAABAEuEIAAAAACQRjgAAAABAEuEIAAAAACQRjgAAAABAkuSyugMT0as7D+vxF6oVaO2SN8etqy8q1YcXTLG6WwAAAABOgHA0yl7deViPPLVHwVBEkuRv7dIjT+2RJAISAAAAJpXXD7+tP1c/reauZuW58/Tx0hU6d8qZVndrUEyrG2WPv1AdC0a9gqGIHn+h2qIeAQAAAMn3+uG39Zs9W9TU1SxTUlNXs36zZ4teP/y21V0bFCNHo8zf2jXo7ZGIKYfDSHKPAAAAgLHXHQkp0Nkkf0dA/s6A/vTuk+qOdPdp060/Vz9t29EjwtEo8+W4Bw1I/98PXtLCEp8Wlfq0cI5X2RmpSe4dAAAAcHIiZkTNXS3ydwTUGBeCon82qaWrVabMIZ+nqas5Cb09OYSjUXb1RaUJa44kKcXl0IWLitUZDGtHjV/bdtXLkFQyNUflpdGwNLMoWw6DUSUAAABYwzRNHetuU2NC6AnI39Gkxs6AmjqbFTbDsfaGDOW5c+VL92ieZ6586V7lp3nlS/fKl+bR/33rwQGDkMedl8yXNSKGaZpDx7txxO8/pkjE2pd0ot3qIqapA4ePqqrar6pqv/bXtcqUlJOZqvISrxaV5mvBbI8y0lIsfQ0AMFEUFGTryJGjVncDAGyhI9Shxo6mfuGn9/tgn2lw2SlZsbCTGH688qTlyuUYfKyld81R/NS6FEeK1s5fY9m0OofDkM+XNej9hKMxNJxfyK1tQf2zNhqUdtYG1NYZksMwNHd6rhb1jCpNy8+UwagSAJwUwhGAyaQ73C1/5/Gw09gn/LSHOhLapznT5Ev3JIQeX7qn50+v3M5TWwZit93qCEcWGukv5HAkoppDraqq9mtHtV8HG45Jkrw5bi0q8am81KeyWR6lpTIbEgCGi3AEYCIJR8LRdT+dgQFGgAJqCSbWO5fDFR31iZvuFj8ClOFKT8qH8HapxYQjC53qm6DpaJd21PSMKu0PqCsYlstpaN6MPJWX5mtRqU9FnuS8oQFgvLLLL2QAGA7TNNUaPNoTfhJHffydTWrqalbEPL623WE45HHnxoWf6MhPfs/X2alZchjWX73HLrWYcGSh0XwThMIR7XuvWVU9YanO3y5JKsxLj23qMH9mnlJczlE5HgBMFHb5hQwAvdq72/tNd+v9PtAZUHcklNA+JzX7eOhJCEFeedy5cjrsf/5nl1pMOLLQWL4JjjR3xEaV9hxoUjAUUarLobJZHi0q9am8xKf8vPQxOTYAjCd2+YUMYPIIhoNq7Ago0NnUE3qioz690986Qp0J7TNc6bHpbvHT3/LTvfKmeZXqHP8bddmlFg8Vjli8Mk4V5KVr6ZnTtfTM6Qp2h7XnYLN2VPv1j+pG/aPaL0mamp8ZW6t02vRcuZzWD6kCAACMd+FIWIHO5j6jPscD0NHuYwntUxwpPet8PCrNm504BS7No4wUPtC2C0aOxpAVCdk0TR0OtGtHtV9VNX7tPdiscMRUWqpTC2Z7Vd4zquTJdie1XwBgFbt8Wglg/IiYEbUGj/as+Unc7rqxI6DmrpaEi506DIe87ryEkZ/83pGgdK+yU7Im/Rpxu9RiRo4mGcMwVOzLVLEvU5eeO1MdXSHtOdAUW6v01jtHJEkzi7KiW4WX5Ktkao4cjsn9DxYAAEwepmmqrbs9FnYSp78FFOhsVqjPup/c1Bz50r2am1ei/HRPwuhPnjtnXKz7wdAYORpDdknIvUzT1AdH2qJB6d1GvftBqyKmqcw0lxaW+LSoxKcFJV7lZJzafvYAYCd2q8UAkqMz1JWwy1v89LdAZ5M6w10J7TNTMuJGfRKv9eN15yllAqz7sZJdajEjR4gxDEPTC7M0vTBLl583S22d3dpZG9COar921Pi1bVe9DElzpubE1irNmpItxyQfBgYAAPbTHQmpqbNJ/o7EUZ/e6W/HutsS2qc6U2Oh53RPaWzUJ7rpgUfprjSLXgnshJGjMWSXhDwcEdPUgcNHY2uVag+1ypSUk5Gi8hKfFs3N14LZHmWk8akJgPFlPNViAMdFzEj0YqcdATX2jPwEOpui64A6A2rpak1Y9+M0nPKm5fUf/ekJQVkpmZN+3Y+V7FKL2crbQnZ5E5yM1vagdtYEVFXj1z9r/GrrDMlhGJo7PbdnrZJP0wooMgDsbzzXYmAiM01Tx7rbYmEnfuSnsTOgps5mhc1wrL0hQ3nu3ITpbvlx217nunNscbFTDMwutZhwZCG7vAlOVTgSUc2hVlVV+7Wj2q+DDdHtKT3Z7lhQKpvtUVoqszQB2M9EqcXAeNQR6owLPcdHgPyd0XVAwXAwoX1WSma/0NM78uNNy5PLwbnGeGWXWkw4spBd3gSjrelol3bURIPSzv0BdQbDcjkNnT4jT4tK87Wo1KciTzqjSgBsYaLWYsAOusPd0c0OEq73c/z79lBHQvs0pztuu2tPbM1PNPx4lObiUiMTlV1qMeHIQnZ5E4ylUDiife+3xC5AW+dvlyQV5qWrvNSnRaU+zZuRp9QUtrcEYI3JUIuBsRKOhKPrfjoDaowLPb1/tgQT/225HK7oaE+fkZ/8NK+86R5lujL48HSSskstJhxZyC5vgmQ60tyhHT3XVNpzoEnBUESpLofmz/LEpuDl53EVaADJMxlrMTBcpmmqNXg0ttV13+lvTV3NipiRWHtDhjxpeQmhJ34kKCc1m3U/GJBdajHhyEJ2eRNYJdgd1t73mlVV7VdVdaOONHdKkop9GdGgVJqv06bnyuWkiAIYO5O9FgPt3e39prv1fh/oDKi7z8VOs1Oz+oWe3ulvHnceFzvFSbFLLSYcWcgubwI7ME1T9U0dPZs6NGrve80KhU2lpTq1YLZX5aU+lZf45MlmrjGA0UUtxkQXDAf7XeQ0fiSoI9SZ0D7dla78uI0Oeqe/9V7vJ9XJxeAx+uxSi7kILGzBMAxN8WZoijdDl54zQ53BkHbvb1JVzxS8t945IkmaWZgVW6tUMjVHTgejSgCAyS0cCSvQ2dyzw1v/EaCjwWMJ7VMcKbFpbyW5s+VL9ySMBGWkML0dGAwjR2PILgnZ7kzT1AdH2mJB6d33WxQxTWWmubRgjleLSn1aWOJTTgafZAEYOWox7C5iRtQaPBq93k9HYgBq7Aiouasl4WKnDsMhjzuvZ81P3xEgr3JSs9j0ALZjl1rMyBFszzAMTS/M0vTCLF1+3iy1d3Zr5/4mVVU3ake1X6/vbpAhac7UHC0q8am81KdZU7LloPADAMYB0zTVFmpPmO7WGLfrW6CzWaE+635yU3PkS/dqbl6J8tMTd3/Lc+ey7gcYI0kbOaqtrdWGDRvU3NysvLw8VVRUaPbs2QO2ramp0VVXXaW1a9fqjjvuGNFxGDmaWCKmqQOHj2pHtV9VNX7VHmqVKSknI0XlPUFp4RyvMtJSrO4qAJuiFiMZOkNdcdtcNx3/s+e2znBXQvtMV0Zso4P4kZ/8NI+8aR6lOPm9honFLrXYNhsy3HDDDVqzZo1Wr16trVu3asuWLdq8eXO/duFwWJ/+9KdVWFiowsJCwhEStLYHtbMmoKoav/5Z41dbZ0gOw9DcaTk9a5XyNb0gk+kEAGKoxRgNoUhIgc4m+TuaEkZ9eqe/HetuS2if6kjpF3riv093pVn0SgBr2KUW2yIc+f1+LV++XNu2bZPT6VQ4HNaSJUv017/+VV6vN6Htj3/8Y6Wmpqq9vV3t7e2EIwwqHImo9tBRVdU0qqrar4P10QWpnmx37JpKZbM9Sktl9igwmVGLMRwRM6KWrtboup+4EaDe71u6WhPW/TgNp7xpeXHhp2fL654AlJXCB3VAPLvUYlusOaqrq1NRUZGczuj8WKfTqcLCQtXV1SWEoz179ujll1/W5s2b9eCDDyajaxjHnA6H5k7P1dzpubr6wlI1He3SP3s2ddi2q14v/O8huZyGTp+RF1urNMXLlbkBYDIyTVPHutv6hJ9AbCSoqbNZYTMca2/IUK47R740r+Z55sZ2f+u93k+uO4eLnQITkG0+Uu/u7tY3v/lN3XfffbEQdTJOlAStUFCQbXUXJo2CgmydXpKvqz8mdYci2r3frzd3N+jN3fV67G/v6rG/vaspvgydPb9IZ5UVqXxuvtwpLGgFJgNq8eTQ3t2hhmN+NbQ1qqHt+J9HjjWqoT2grlDiup9sd5YKM306LX+2CjN9KszMV2FW9M/8DNb9AKNtPNRi20yrO3TokK666iplZmZKklpbW2Wapi6//HLdc889IzgW0+rQX2Nzh3b0jCrtPtCkYCiiVJdD82d5tKjnArQFeVz3AZiIqMUTR3e4W4HOJjXGXeA0fgSoLdSe0D7N6Y5b5+OJjfp406Jfp7m48DiQLHapxbaYVufz+VRWVqbKykqtXr1alZWVKisrS5hSN3XqVG3bti32/QMPPHBSa46AgeTnpeviM6fr4jOnqzsU1t6DzfpHtV9V1dH1SpJU7MuIrVU6bUaeXE6mSwBAMoUjYTV3tfRc36epT/gJqCWYeGLlMpzy9oSemTkz4i50Gp0Cl+liKjWAkUnatLqNGzdqw4YNevDBB5WTk6OKigpJ0rp167R+/XqVl5cnqyuY5FJcTi0siV5Ydu3HTlN9U4eqqv3aUd2o5956X8+8/p7cqU4tmO2NjSp5svl0EQBOlWmaag0e6xd6ekeCmrqaFTEjsfaGDOW5c5Wf7lWZd17C1tf56V7lpGaz7gfAqEraVt7JwrQ6nIrOYEi7DzRpR7Vf/6j2q+lodH76jMKs6KhSqU8lU3PkdPDLGBgvqMXJ1d7drsbOgAKxLa+bEnZ/6450J7TPTs2KG/FJnP7mcedxsVNggrBLLbbFVt7JRDjCaDFNUx80tkUvQFvt1773WxQxTWWmubRgTnRUaWGJTzkZqVZ3FcAJUItHVzAcjF3ktLHfRU8D6gh1JrRPd6XFXejUE7ftdfT7VCc1FJgM7FKLbbHmCBiPDMPQ9IIsTS/I0mXnzVJ7Z7d27m9SVXWjdtQE9PruBhmSZhfnxEaVZk3JloP57QDGsXAkrKau5rgtr5vipr8FdDR4LKF9isMlb8+IT0nurIQRoPw0rzJSMix6JQAwcowcjSG7JGSMvohp6mD90Z61Sn7VHGqVKSknI0XlPddUWjDHq8w0toEFrEYtThQxI2oNHo2Gn74BqLNJTZ3NCRc7dRgOedx5PSM+nn7T33JSs9n0AMCQ7FKLmVZnIbu8CTD2jrYH9c/agKqq/fpnjV9tnSE5DENzp+WovNSnRaX5ml7A1dIBK0y2WmyaptpC7QnT3RrjNkAIdDYrFAklPCY3NTsu9PRe6DQafvLcuaz7AXDK7FKLCUcWssubAMkViZiqOdSqqproNuEH66NTUDzZbpWXRKfflc3yKN3NrFYgGSZiLe4MdSnQ2dSz5XX/6W+d4cSLnWa40vtNd+v93pvmUSoXOwUwxuxSiwlHFrLLmwDWajrapX/W+FVV49fO2oA6g2E5HYZOn5EXW6s0xcu1OICxMh5rcSgSioaf2I5viQHoWHdbQvtUR0rCyE/v9DdvzwhQuouLXAOwll1qMeHIQnZ5E8A+QuGI3n2/RVU10bVKHzRGT3AK8tK0qCRf5aU+zZ+Zp9QUprAAo8WOtThiRtTS1Ro36hOdAtf7fUtXa791P9603hEfT5/pb15lpTBtF4C92aUWE44sZJc3AeyrsblDO2qiW4XvPtCkYCiiFJdDZbM8sSl4BXl84gucCitqsWmaOtbddjz49BkBCnQ2K2yGY+0NGcp158S2uu47ApTnzuVipwDGNbucFxOOLGSXNwHGh+5QWHsPNquq57pKDc0dkqRiX0YsKJ0+I08uJydIwEiMVS3uCHXGTXcLqDHuWj/+ziYFw8GE9lkpmQm7vMVf98eb5lGKg3WIACYuu5wXE44sZJc3Acan+kB7NCjV+LX3YJNCYVPuVKcWzI5egLa8xCdPttvqbgK2d7K1uDvcrUBnU2LoiVv70xZqT2jvdqbGjfgkhh9fmkdprrTRekkAMO7Y5byYcGQhu7wJMP51BkPac6BZVdWNqqrxK9Aa3YlqRmFWLCiVTsuR08GoEtDr9cNv68/VT6u5q1l57jx9vHSFzp1yZuz+cCSs5q6Wnh3fmhTo+bM3BLUEWxOez2U45Y0LO/FrfnxpXmWmsLEKAAzGLufFhCMLDPULGTgVpmnqg8Y27eiZfrfv/RZFTFMZbpcWlnijF6Et8SknM9XqrmIC6/3VEb9pQPyvk97bzegdx7+O+//x9gPcb/Zp23scs88xB2xv6n8b/qk/Vf9F3XHX83EaDpXkzJLhcMrfEVBTV7MiZiR2vyFDee7cWNjpO/0t153Duh8AOEmEI4tYHY5eP/y2frNni7oj3bHbXIZLl866WGW+0xX/i3/gk4qBTxIGOhmIP0lIbD9A27jb+/3fHOAEZLA+Ddn2RCdFvfcO1f/EW+Lb93tNMkfUp/jnSLjXjG859Eld39d1ouMOdVI3vD4N/HolU6FwRE1HuxRo7ZL/aKe6Q9FF3tnpKfLkuOXJdiszPUVG3HMM9HoH73/sjoF6n3C72fO3EPcjGqD//Z7hhMfs13aQn80Jf2bRLwa6dYD+D9wn9b6yAd8Tia9rsJ9Z7/+H+jc6rJ/ZiP4dDd2nEf07GscMGZqVMyM28hN/vR9PWq5crPsBgDFBOLKI1eHoG6/cq6auZsuOj7FlKHHKTPwUGiP+XiO+5cC3R78yem/u19aQ0Xu3+j1b3O3x90hSOGIqFIooGIooHI6ezjoMQ6kuh1JTnEpxOeUwTtSn6FMNcOvAbQf4OcR6Y8R9Hfv/4P2P/zn0+78xwM9hhH2Kf119nzu+f4P9PQ70uoY8ZtzrGrj/w+lT/DGHbjvw32Pi7cdf68n+Pfa9faD39DD61GcaWv+f/0h/ZtHvH9v7uAbzo6X/Z9D7AABjY7yEIz4iG2UnCkaf+5fPDOvEZaAT5egJ1mBtBz9JONFJdcKJy4AndcdvH/DefgHgJPrU77hDnQj3PVrPd0P8zPr/P7H/J+rTeF5DcLQ9qH/WBrSj2q8de/xq6QzJMKS503Jja5VmFGaN69cIDOSZ/X8bsB573HkW9AYAMF4QjkaZx5036C/kBb55FvQIk1l2Rqo+vGCKPrxgiiIRUzV1raqqjl6AdssLNdryQo082e7YVuFlszxKd1MWMP59vHRFvynOKY4Ufbx0hYW9AgDYHdPqRtlAa45SHClaO38NmzLAVpqPdcUuQLuzNqDOYFhOh6HTZ+RpUWk0LE3xsvsWxi82xwEA+xgv0+oIR2OAX8gYb0LhiN59v0VVNdFRpQ8a2yRJ+blpsaA0b6ZH7hSnxT0FRs4uv5ABYDKzSy0mHFnILm8CYKQaWzq0oya6VmnXgYCC3RGluByaP9MTXatU6lNhXrrV3QSGhVoMANazSy0mHFnILm8C4FR0h8La+16zqnquq9TQ1CFJKvZlxNYqnT4jTy4n13+BPVGLAcB6dqnFhCML2eVNAIym+kB7NCjV+LX3YJNCYVPuVKfOmOWJ7YDnzUmzuptADLUYAKxnl1rMVt4ARlWRN0OXeDN0yTkz1BUMa/eBJlXV+FVV3ajt+xolSdMLsmJrlUqn5cjpYFQJAADYHyNHY8guCRlIBtM0daixLRqU3vVr3/stipimMtwuLSzxqrwkOqqUk5lqdVcxyVCLAcB6dqnFjBwBSArDMDStIEvTCrJ02ZJZau8Madf+QPS6SjV+vb67QZI0pzi7Z61SvmYXZ8vBVuEAAMAmGDkaQ3ZJyIDVIqap9+qPqaq6UVU1ftV80CpTUnZGihbOiU6/WzDHq6z0FKu7igmIWgwA1rNLLWbkCIDlHIahWVOyNWtKtladP0dH24PaWRuIXlepxq9Xdx6WYUhzp+XGNnWYUZjFBWgBAEBSMXI0huySkAE7i0RM1dS1RqffVft1oD76byYvK7UnKOXrjNkepbv5LAcnh1oMANazSy1mK28L2eVNAIwnzce6tKMmGpR27g+ooyssp8PQ6TPyYtdVKvZlMKqEYaMWA4D17FKLCUcWssubABivQuGIqj9oiV1X6YMjbZKk/Ny02Fbh82Z65E5xWtxT2Bm1GACsZ5daTDiykF3eBMBE4W/pjK5TqvZr14GAgt0Rpbgcmj+z5wK0pT4V5qVb3U3YDLUYAKxnl1pMOLKQXd4EwETUHQpr73vNsbVK9U0dkqQp3oxYUDp9ep5SXFyAdrKjFgOA9exSiwlHFrLLmwCYDOoD7bFRpT0HmxUKR+ROdeqMWZ7YDnjenDSruwkLUIsBwHp2qcVs5Q1gUijyZugSb4YuOXuGuoJh7T7Y1DOq1Kjt+xolSdMLsmJrlUqn5cjpYFQJAAAcx8jRGLJLQgYmM9M0daixLTaqtO/9FoUjpjLcLi2Y49WiUp8WlviUm5lqdVcxRqjFAGA9u9RiRo4ATGqGYWhaQZamFWTpsiWz1N4Z0q79gVhYemNPgyRp9pTsnlGlfM0uzpaDrcIBAJh0GDkaQ3ZJyAAGFjFNvVd/TFXVjaqq8avmg1aZkrIzUrRwTnT63YI5XmWlp1jdVZwCajEAWM8utZiRIwAYhMMwNGtKtmZNydaq8+foaHtQO2t7RpVq/Hp152EZhlQ6LVeLei5AO6MwiwvQAgAwQTFyNIbskpABjFwkYqq2rjV2AdoDh6P/lvOyUlXeE5TOmO1VupvPmOyOWgwA1rNLLWYrbwvZ5U0A4NS1HOvSjproqNLOWr86usJyOgydPiMvFpaKfRmMKtkQtRgArGeXWkw4spBd3gQARlcoHFH1By2xUaUPjrRJkvJz01Re6tOiEp/mz/LIneK0uKeQqMUAYAd2qcWEIwvZ5U0AYGz5Wzq1o8avqmq/dh0IKNgdkcvp0PxZedG1SnPzVZiXbnU3Jy1qMQBYzy61mHBkIbu8CQAkT3corHfeOz6qVB9olyRN8WZoUalP5aU+nT49TykuLkCbLNRiALCeXWox4chCdnkTALBOfVO7qqqj11Tac7BZoXBE7hSnzpjtiU3B8+akWd3NCY1aDADWs0stZitvALBQkSdDl5ydoUvOnqGuYFi7DzZpR7VfVdWN2r6vUZI0vSAzFpRKp+XK5WRUCQAAKzByNIbskpAB2I9pmjrkb48FpX3vtygcMZXudmnhHK8Wlfq0sMSn3MxUq7s67lGLAcB6dqnFjBwBgA0ZhqFp+Zmalp+pFUtmqqMrdPwCtNV+vbGnQZI0e0p2bK3SnCk5cjjYKhwAgLGStJGj2tpabdiwQc3NzcrLy1NFRYVmz56d0GbLli16+OGH5XA4FIlE9IlPfEI33HDDiI7DyBGA8c40TR2sPxYLStWHWmSaUlZ6ispLvCov9WnhHJ+y0lOs7uq4QC0GAOvZpRbbZkOGG264QWvWrNHq1au1detWbdmyRZs3b05oc+zYMWVmZsowDB07dkyrVq3Sj3/8Y82fP3/YxyEcAZhojnV065+10aC0oyagYx3dMgypdGquFpVGL0A7ozCLC9AOgloMANazSy22xbQ6v9+vXbt26Ve/+pUkaeXKlbrnnnsUCATk9Xpj7bKyjne0s7NT3d3d/LIHMOllpafovDOm6LwzpigSMVV7uFVV70a3Cn/8xRo9/mKNcrNSo9dUKvXpjNlepbuZNQ0AwEgl5bdnXV2dioqK5HRGrxbvdDpVWFiourq6hHAkSc8995z+67/+SwcPHtTtt9+uefPmjehYJ0qCVigoyLa6CwAmmKKiHJ33L9MlSU2tnXprT4Pe3FOvt/c26KWqOjkdhhaU+HTW/CKdXVaoGUXZk/6DJmoxAFhvPNRi2320uGzZMi1btkyHDh3S5z//eV144YUqKSkZ9uOZVgdgsvmXOR79yxyPQpeeruoPWmJrlX5VuVO/qtyp/Ny02Fbh82d55E5xWt3lpKIWA4D17FKLbTGtrri4WPX19QqHw3I6nQqHw2poaFBxcfGgj5k6darKy8v197//fUThCAAmK5fToXkzPZo306NPfHSuAq2dqqr2q6rar1d21On5tz+Qy+nQ/Fl5sSl4hZ4Mq7sNAIBtJCUc+Xw+lZWVqbKyUqtXr1ZlZaXKysr6Tamrrq5WaWmpJCkQCGjbtm269NJLk9FFAJhwvDlp+ujiafro4mnqDkX0znvN0bBU49dvnt2n3zy7T0XejFhQOn1GnlJcXIAWADB5JW23uurqam3YsEGtra3KyclRRUWFSkpKtG7dOq1fv17l5eW699579corr8jlcsk0TX3iE5/Q9ddfP6LjMK0OAIZW39RzAdoav/YcaFYoHJE7xamyWZ7YDnjenDSruzkqqMUAYD271GLbbOWdLIQjABiZru6w9hxoUlWNX1Xv+uVv7ZQkTS/IjK1VKp2WK5dzfI4qUYsBwHp2qcWEIwvZ5U0AAMNlmqYO+XtGlaobte/9FoUjptLdLi2Y49WiEp/KS7zKzXJb3dVhoxYDgPXsUottsSEDAGB8MAxD0/IzNS0/UyuWzFRHV0i79gdia5Xe3NMgSZo1JTu6VmmuT3Om5MjhmNxbhQMAJgZGjsaQXRIyAIwG0zT13v/f3n2HR1XmbwO/z5leMiVT0iEQWgygCBZQRLHh6ouKgg0X9X0tP8CCa1t3F1RgV9hXUS7D2hAQlVUQRYooFpCqIig1KgktIWXSCDNJZjLl98dMTmYIoUiSmWTuz3V5GZ455TlDrsN853me+5Q5pUIpv+goAoHgQ2r7dU9EvywL+nazQK9RRLurEXgvJiKKvli5F3PkiIiIWoUgCOiSlIAuSQm4YUgmnHUN2LU/+EylnQWV2Ly7FIIAZKUapbVKXZL0cf8AWiIi6jg4ctSGYqVCJiJqa35/APtLakJrlSpwoCR47zPqlejXPVgo5XRLhEbV/t/J8V5MRBR9sXIvZiBDFMXKVH9KlAAAIABJREFULwERUXs76vJgV0GwUNq1vxJ1bi9kooCe6Ub0z7KiX5YFqRZtu4wq8V5MRBR9sXIvZnEURbHyS0BEFE1enx/5RUexoyA4Ba/Q4QIAWAxq9M+yoF+WBdldzFApZW1yft6LiYiiL1buxSyOoihWfgmIiGJJZU29VCjtOVAFd4MPcpmIPl1MwbVKWRYkmbWtdj7ei4mIoi9W7sUsjqIoVn4JiIhiVYPXj98Kq6W1SiWVtQCApERtMCo8y4JeGSYo5H/8AbS8FxMRRV+s3ItZHEVRrPwSEBF1FKVVoQfQFlQg72A1vD4/lAoR53RNDE7B626Bxag+o2PyXkxEFH2xci9mlDcREXUYSWYtkgZpcdWgDLgbfMg7WIUdBRXYsa8CP+8rBwCk2XTSqFJWmhFy2R8fVSIiIgrHkaM2FCsVMhFRRxcIBFBcUYsd+RXYWVCB3w5Xw+cPQKOSISczMZiA1z0RRr1K2mfz7hIsXZePyho3Eg0qjBqWhcE5yVG8CiKi+BUrn4s5rS6KYuWXgIios6lze7HnQCV2hKbgHXV6AABdkxPQv7sFogB8/v0heLx+aR+lXMS46/qwQCIiioJY+VzMaXVERNTpaFRyDOxtx8DedgQCARwuc0qF0orNB3Cir/08Xj+WrstncURERC3iyFEbipUKmYgonjjrGvDIq+tbfL1fdwvS7Tqk2/TIsOmRbNFy3RIRURuLlc/FHDkiIqK4otcoYDGoUFHjbvaaUiGi6pgbew5Uwhf6Ik0mCkixaJFuDxZLaTY9Mux6mPRKCILQ3t0nIqIoYnFERESdzqhhWVjweV7zNUcjgmuOvD4/SiprUVjmxGGHE0UOF349VI0tu0ul7XVqOdJt+mDRZNcjzaZDulUPlVIWjUsiIqJ2wOKIiIg6ncZ1RS2l1cllYrDwselxcdh+zroGFDmcKHS4cLjMiSKHExt2FMPd4AMACABsZk1oXx0y7MFj2EwaiCJHmYiIOjquOWpDsTK3kogonp3tvdgfCKC8ug6FDpc00lTocKGsshaN/9ooFSLSrMGCqXF6XrpdD71G0ToXQUTUwcXK52KuOSIiIjoLoiDAbtbCbtbi/F42qd3d4MOR8rCCqcyJ7b+XY/2OYmkbk16J9NDoUmPBlMIACCKimMXiiIiI6A9QKWTolmJAtxSD1BYIBHDU5UFhWdPUvEKHE3kHD8PrawqASLZoQ+EPTVPzzAkqBkAQEUUZiyMiIqJWIggCTHoVTHoV+na3SO1enx+llbVS+MPhMid+K6zGlj1NARBalbwpMc+uk4ontZL/VBMRtRfecYmIiNqYXCYiLRQTHq62viEi/OGww4kNu4rh9vikbWwmdXBaXmiEKd2uh50BEEREbYLFERERUZRo1Qr0yjChV4ZJavMHAqg4Wh8R/lBY5sTP+8rRGKGklItItYaFP4SCIBK0yihdCRFR58DiiIiIKIaIggCbSQObSYMBYQEQngYfjlSE1jGVuVDocOLn38uxISwAwqhXhoU/6JBu0yPFooNCzgAIIqLTweKIiIioA1AqZMhMNiAzOTIAosblaTY176ufmgIgREFAikUrhT+khYqnRAMDIIiIjsfiiIiIqIMSBAFGvQpGvQo53RKldp/fj9LKOhQ6nKGiyYX8ohr8sLdM2kajkkc+lykUAKFR8aMBEcUv3gGJiIg6GZkYXJOUatXhwuwkqb223ouicmdoPVNwat7mXSX4NiwAwmpUS8EPwRAIHZLMWgZAEFFcYHFEREQUJ7RqOXqmm9AzvSkAItAYAOFwSQ+zLXQ48Ut+UwCEojEAwqaTHmabbtPDoGMABBF1LiyOiIiI4pggCLCaNLCaNDivp1Vq9zT4UFxRK03NK3Q4sbOgEht3lkjbGHRKZNh0wXVMoYIp1aqFQi6LxqUQEZ01FkdERETUjFIhQ9fkBHRNTohoP+ryoNDhRFFj1HiZC99sK4LX5wcQDIBIStREhD+k23WwGNQMgCCimMfiiIiIiE6bUaeEUZeInMzIAIiyqrrQCFPwuUwFR44PgJA1FUuhIIh0m54BEEQUU3hHIiIiorMiE0WkWHRIsehwYXZTe53bi6LGtUyh9Uxb9pSgzt0UAGExqINT8kLPZUq36ZGUqIFM5LOZiKj9sTgiIiKiNqFRydEj3Yge6UapLRAIoLLGHRH+UOhwYUd+BfyhBAi5TESqVRsR/pBu18PIAAgiamMsjoiIiKjdCIIAi1ENi1GN83o0BUA0eIMBEIfDCqZd+yuxcVdYAIRWERH+kG7XIdWig1LBAAgiah0sjoiIiCjqFHIZuiQloEtSZABETa0nFP7gkkaavt1ehAZvMABCEIDkRG1oPVPTWiaLUQ2RARBEdIZYHBEREVHMMmiVMGQmIjssAMLvD6C0qja4nilUMB0sqcHWvKYACLVSFlrD1FQwpdv00Kr50YeIWsY7BBEREXUooihIARCD+til9jq3F0XlLin8odDhwg97y7D25yPSNhaDSlrD1Pj/ZAZAEFEIiyMiIiLqFDQqOXqkGdEjLTIAouqYO+xhtsHiadf+Svj8jQEQAlItYSNMdh0ybHoYdEo+m4kozrA4IiIiok5LEAQkGtRINKjRPys8AMKP4gpXU9R4mRN7DlRiU1gAhF6jaAp/CE3PS7XqoGIABFGnxeKIiIiI4o5CLkoBEIPD2o/VeqTRpcYAiHU/F8ETFgBhN2sjwh/S7XpYGQBB1CmwOCIiIiIKSdAqkd1VieyuZqnN7w/AUV0XETN+qNSJn351IBDaRqWUId0aHv4Q/FmnVkTnQojoDxECgUDg1Jt1HBUVTvj9kZfk83lRVeWA1+tp176Iogi/39+u54wVcrkSZrMNMhnrbyKKLpstAQ7HsWh3gzqhek8oAKJxLVOoeHLVe6VtzAmqZlPzkhO1kMsYAEHxJVbuxaIowGLRt/h6XHxyrapyQK3WQqdLbteFlXK5CK83/oqjQCAAl6sGVVUOWK0p0e4OERFRm1Ar5chKNSIrNTIAotrpCRtlCk7P2x0WACETBaRadVKxlGHTI82mh0nPAAiiaIuL4sjr9bR7YRTPBEGATmeA01kd7a4QERG1K0EQYE5QwZygQv8si9Tu9flRUlErhT8UOlzIO1SNzbtLpW30GkWwYAqtY8pgAARRu2u34mj//v145plnUF1dDZPJhBkzZiAzMzNim9zcXKxatQqiKEKhUGDSpEkYOnRoq5yfhVH74vtNRETURC4Tg+uR7Hogp6ndWdeAIilmPFg0fbfjCDwNoQAIAHazJuJBthl2HawmDQMgiNpAuxVHU6ZMwZ133okbb7wRy5Ytw+TJk/Huu+9GbNO/f3/cd9990Gg0yMvLw9ixY7Fhwwao1er26iYRERFRu9FrFOjdxYzeXcICIALBAIjCsrDUvDIntoUHQChkSLPppJGmDHtwap5ewwAIorPRLsVRRUUF9uzZg3nz5gEAbrjhBkydOhWVlZVITEyUtgsfJerdu3dw3m51NZKTk9ujm+3i/vvHoaGhAV5vAw4fPoRu3bIAAL169cazz0455f6ffroEbrcbt91210m327BhHX755WdMmPBoq/SbiIiI2ocoCEgya5Fk1mJgb5vU7vb4ggEQYTHjP/3qwHe/FEvbmBNUEeEPGTY9ki0MgCA6Xe1SHBUXFyMpKQkyWXDOrEwmg91uR3FxcURxFO7TTz9Fly5dzrgwOlH6RFmZCLn8zG4Km3YVY/G3+ag4Wg+LUY3RV2RhSN8zDxc4/rzz5i0EABw5cgT33jsW773334jXvV4v5PKW/1puvXXMaZ338suvwOWXX3GGvW1doijCZkuIah+IiADwXkSdRnqaCReF/TkQCKCyph4HimtwsLgG+4trcOBIDdZsPQyvLzjOJJcJSLcnIDPFgK4pBmSmGNAt1YBEg5rT4KlddYR7cUwGMvzwww949dVX8c4775zxvieK8vb7/WeUGrd5dwkWfJ4nPfCt4mg93lmxFz5fAINzTr9YO1lanc/nBxCA1+vHrbf+H1x55TXYtu1HdO/eAw88MB7PPfc3uFwueDweDBlyCcaPD44AzZ37Burq6jBx4mNYtWo51qxZjYQEAwoK8pGQoMe0aTNhsVixatVybNq0HtOmzcS2bVsxe/bLOOecHOzevROAgOef/ycyM7sBAN54IxfffLMGBoMRAwYMxE8//Yi5cxee9nW2xO/3x0RkIxHFt1iJjyVqS10sWnSxaDG0b/BzitfnR0llbVPMuMOJHfscWLutUNpHp5ZL4Q+NI03pVj1USgZAUOuLlXtxTER5p6SkoLS0FD6fDzKZDD6fD2VlZUhJaT4Ss337djz55JOYM2cOunfv3up92bizGBt2FJ90m/wjR6VvWxp5vH7MW7UX3/18pMX9Lu2fgkv6/bHoapfLhbfeCq7BcrvdmDFjFrRaLbxeLx5/fCK2bNmEiy8e0my/vXv3YMGCRUhKSsaMGdOwZMmHePDBCc22278/H88+OxlPPfU3LFgwFwsWzMWUKdOwYcN32LRpA+bPXwSVSoW///3pP9R/IiIiih1ymSgFOIRz1TdEFEyFZU5s2FkMt8cHIBgAYTNpmgqm0Homm0kDUeQoE3V+7VIcWSwWZGdnY8WKFbjxxhuxYsUKZGdnN5tSt2PHDkyaNAmzZ89GTk5OC0dre8cXRqdqbw0jRlwv/ez3+zFnzqvYuXMHgAAqKirw+++/nbA46t//XCQlBb8lysnpix9//P6Ex+/SpSt69eoT2q4fNm5cDwDYvn0rhg+/ChqNBgBw3XXXY/78ua15aURERBQjdOoTB0CUH62X1jEVljlx2OHC9t+aAiCUChFpVl3YSFOwaGIABHU27Tat7rnnnsMzzzyDOXPmwGAwYMaMGQCA+++/H4888gj69euH559/HvX19Zg8ebK038yZM9G7d+9W68cl/U49uvPknI2oqHE3a7cYVHj6rvNbrS/htFqN9POHH76PY8dq8Oab86FSqTBjxnR4PM37AwBKpVL6WRSDo3In3k4Vtp3Y4nZEREQUX0RBgN2kgd2kwfm9wgIgGnw4Uu6KGGna/ns51ofNwDHplU3PZbLpkWbTIcWig+IM13oTxYp2K46ysrKwePHiZu1vvfWW9PPHH3/cXt05qVHDsiLWHAGAUi5i1LCsdjn/sWPHYLFYoVKp4HCUYcOGdbjpplva5FwDBgzE3Llv4Lbb7oJSqcQXX6xqk/MQERFRx6JSyNAtxYBuKQapLRAIoMblCT3Mtmlq3ldhARAyUUCyRSul5mWERprMCSoGQFDMi8lAhmhrDF1Yui4fFTVuWAwqjBqWdUZhDGdj9Ojb8Y9/PI277x4Dmy0JAwde0GbnuvTSYdi5cwfGjbsdBoMBOTn9cOxY9BfLERERUewRBAFGvQpGvQp9u1mkdq/Pj9KquoipefsKq/H9nlJpG61KHhH+0DjSpFby4yjFDiEQCLTdQpooOFFaXUnJQSQnd233vpwsrS6W1Na6oNXq4Pf78eKLU2G12vDAA+PP+rjRet+JiMLFSkISUTyqrW+ICH8odLhw2OGUAiAAwGZSS+ERGfbgFD07AyA6nVi5F8dEWh3FtqlTp6Ck5Ajcbjd6987GXXf9OdpdIiIiok5Aq1agV4YJvTJMUps/EEDF0fqI8IcihxM/7ytH41f2SrmIVKuuKfzBpkOaXQ+DVtnCmYhaB0eO2lBHGTlqKxw5IqJYECvfVhLRyXkafDhS4ZLWMh0OTdE7VtsgbWPUKSOm5GXY9QyA6CBi5V7MkSMiIiIiinlKhQyZyQZkJhsi2o+6PMfFjDvx1U+F8PqCX0CLQmMARLBYSrMFi6dEAwMg6MyxOCIiIiKimGXUKWHsloicbk3Px/T5/SitrAsWTKHkvPyiGvywt0zaRqOSR4Q/pIdGmzQqfvyllvG3g4iIiIg6FJkYXJOUatXhwuwkqb223oui8sjwh827SvBtWACE1ahuejZTKD3PbtZAJnJqHrE4IiIiIqJOQquWo2e6CT3TmwIgAoEAKmrqUVgWLJaKQuuZduRXwB9aeq9oDICw6YLrmUKjTQYdAyDiDYsjIiIiIuq0BEGA1aiB1ajBeT2tUnuD14cj5bVhU/Oc2FlQiY07S6RtDDplcGpeY8y4TY9UqxYKuSwal0LtgMVRC34o2YbP8lejyl0Ns8qEkVkjcGHy+Wd93L/85REMHXoZbrrpVqktEAhgzJib8OyzkzFgwMBm+0yf/hz69MnGLbfchk8/XQK3243bbrur2XarVi3Hpk3rMW3azJP24bvv1sJqteKcc/oCAPLy9uDDDz/AlCnTzvLqiIiIiDoGhVyGrskJ6JqcENFe4/I0ey7Tt9uL0OBtCoBIStQ0Tc2zBafmWYxqBkB0AiyOTuCHkm34IO9jNPiD0ZFV7mp8kPcxAJx1gXT99SPx3/++F1Ecbd/+E0RRwHnnnfrY4fv9UevXr0WfPtlScdSnzzksjIiIiIgQHC06R5eIczKbAiD8/gBKq2qDxVJZcGre/uIa/JgXHgAhQ5qt6blM6XY90qx6aNX8uN2RxN3f1vfFP2Fz8Y8n3Wb/0UPwBrwRbQ3+Bry/dwk2Hfmhxf0Gp1yAi1Kaj/yEGzp0GF566V84cGA/MjO7AQBWrvwM1177J0yYcD/q6+vg8XgwcuTNGDPmzmb7z537Burq6jBx4mNoaGjArFkzsW3bVhiNJvTs2VvaLj9/H1566cVmx/v++83YsOE7bN36A5YvX4bbbrsTSUnJyM19FXPnLgQAfP75CixatBCCICA1NR1PPfUszOZErFq1HGvWrEZCggEFBflISNBj2rSZsFiszfpJRERE1FmIooAUiw4pFh0u6GOX2uvcXhSVu6SI8aIyJ77fU4q17qbPkRaDOhQxrpOm5iUlMgAiVsVdcXQ6ji+MTtV+JhQKBa6++jqsWvUZxo9/FLW1Lqxfvw4LF36IsWPvgVKpRG1tLR54YBwuvHCwVECdyLJlH6O4+Ajee28xvF4vJky4HykpKQCAlJQUvPLKnGbHu+iiwbj00sukaXoAsG3bVumYBQX78Prrr2Hu3PdgtVrx1lv/waxZ/8YLL/wLALB37x4sWLAISUnJmDFjGpYs+RAPPjjhrN8XIiIioo5Go5KjR5oRPdKMUlsgEEBljVtayxQcaXJFBEDIZSJSrdrQw2xD65nsehgZABF1cVccXZQy8JSjO3/f+E9UuaubtZtVJjx2/kNn3Yfrrx+JJ554GA8+OBFff70G/fqdC4VCgRdfnIp9+36DIIgoL3dg377fTlocbdv2E6677gbI5XLI5XJce+112LHjZwBAfX09XnvtxTM6XvCYWzF48CWwWoOjQTfeOAr33NM0gtW//7lISkoGAOTk9MWPP35/tm8HERERUachCAIsRjUsRjXO7REeAOFHcYVLei7TYYcTuw5UYuOupgCIBK1CCn9oHGlKteigVDAAor3EXXF0OkZmjYhYcwQAClGBkVkjWuX4PXv2gsViw5Ytm7Bq1WcYPfpOvPFGLhITLXjnnfchl8sxadIEeDyeP3yO1j5eI6Wy6RsNUZTB5/OdZGsiIiIiAoJx4V2SEtAl6bgAiFoPisLCHwrLnFi7vQieUACEIABJZm0o/EEnBUFYjGqIDIBodSyOTqAxdKEt0uoaXX/9SLzzzpsoLS3G0KHD8PXXXyArqyfkcjkKCvbhl19+xtVXn7wYGzhwEFavXoXhw6+Gz+fFmjWrpVEdp/NYi8fT6XRwOp0nPOb55w/CwoXzUVFRDovFiuXLP8UFF1zYatdNRERERE0MWiUMmYnIPi4Aoqy6LpSYF5yad7CkBlvDAiDUSllwdClULKWHUvO0akU0LqPTYHHUgguTz2/VYuh4V189Arm5r2LkyJuhUCgwbtz/xdSpk7Fy5TJkZHTBeecNOOUxRo4chX379mHs2NEwGk3o0ycHVVUVAHDS41177Z8wffrz+Pbbr6VAhkbdu/fAQw9NxKRJE0KBDGl48slnW/8NICIiIqITEkUByYlaJCdqMei4AIgj5aGH2Yam5v2wtwxrfz4ibWMxqJrWMYUKpqRELeQyBkCcDiEQCK0M6yQqKpzw+yMvqaTkIJKTu7Z7X+RyEd7QkGg8itb7TkQUzmZLgMNxLNrdICJqE4FAAFXH3BHhD4cdTpRU1MLnbwyAEJBq0YWFPwSn5xl1ynZ7NlOs3ItFUYDFom/xdY4cERERERF1UIIgINGgRqJBjf5ZTQEQXp8fxRW1Usx4ocOJvQcrsXl3UwCEXqNoCn8ITc9LteqgiuMACBZHRERERESdjFwmIsMeHCkaHNburGtoei6Tw4nDZS5898sReBpCARAA7InaiPCHdLse1jgJgGBxREREREQUJ/QaBfp0NaNPV7PU5g8E4AgFQDROzTtU5sRPvzrQuFhFpZAh3aaLCH9It+uh62QBECyOiIiIiIjimCgISDJrkWTWYmDvpgAIt8eHonJX2HomJ7bmlWFdWACEOUEVEf6QbtcjOSwAYvPuEixdl4/KGjcSDSqMGpaFwTnJzfoQK1gcERERERFRMyqlDN1TDeieapDaAoEAqp2e0MNsQ+uZylzYvb9SCoCQiQJSLDqolSL2Fx+T2itq3FjweR4AxGyBxOKIiIiIiIhOiyAIMCeoYE5QoV93i9Tu9flRUlEbHGWSCqYKHBciDY/Xj6Xr8mO2OGLgeZTU1NRg+PBL8Mor/z/aXSEiIiIiOitymYh0ux4X5yRj9OU9MGnMuc0Ko0YVNe727dwZYHHUgpotm1Dw1F/w2/+7BwVP/QU1Wza16vHXrFmNnJy++OqrL9DQ0NCqxz6e1+tt0+MTERERER3PYlCdUXss4LS6E6jZsgml785HwOMBAHgrK1D67nwAgOHiIa1yjpUrP8P48Y9g4cL5WL9+HYYPvwoORxleeeXfKCw8DAC46qprcffd98LpdGL27JeQl7cHgiDi3HPPw+OPP43p059Dnz7ZuOWW2wAg4s/Tpz8HmUyGQ4cOora2FvPnf4Dnn/87Dh06iIYGD9LSMvDXv06GwRCcQ7pixTIsXvxfAIBCocDMmbMwb97bSElJwZ13/hkA8NtveZgy5Vl88MHH7fbAMCIiIiLqmEYNy8KCz/Pg8fqlNqVcxKhhWVHs1cnFXXFUs2kjjm747qTb1BfkI3DcaEvA40Hp/Hdw9Lt1Le5nvPQyGIZccso+7Nv3O2pqjmLgwAtQWVmBlSs/w/DhV+GFF/6BwYMvwfTp/wYAVFdXAwBmz34JGo0G8+cvgiiKUvup/P77b3jttTeh0WgAAI8++gRMJhMA4M035+D99xfgf/7nYWzbthULF87DnDlvw2Kxora2FjKZDLfcMgZPPz0Jd9xxNwRBwMcff4Sbbx7NwoiIiIiITqlxXRHT6jq44wujU7WfqRUrlmHEiOshCAKGDbsCs2b9GyUlxdi1awdmzcqVtmssZDZtWo+3334PoihGtJ/K5ZdfKRVGALB69Qp8+eVqeL0NqKurR0ZGFwDA5s0bMWLE9bBYgk9V1mq1AIDMzG5ITU3Dli2bkJPTDxs3foeHH3787N8AIiIiIooLg3OSMTgnGTZbAhyOY9HuzinFXXFkGHLJKUd3Cp76C7yVFc3a5YkWZDz117M6f0NDA776ajUUCiVWr14JILgmaNWq5Wd8LJlMBn/YSjePJ3Jxm1bbVBj98st2fPrpx/jPf96B2WzGl1+uxmefLT3lOW699XZ88skSHDiwH5dddgX0ev0Z95OIiIiIqCNgIMMJWEfdAkGpjGgTlEpYR91y1sdev34dMjK64pNPVmHJkuVYsmQ5Zs16DV9++Tn69u2Pjz76QNq2cfrckCFDsWjRuwgEAhHtaWkZyMvbDQAoLy/Htm0/tXjeY8eOQafTw2g0wuPxYOXKz6TXBg++BKtXr0RlqCCsra2F2+2WXjt06CA+/PB9jBo15qyvn4iIiIgoVrE4OgHDxUOQ9Od7IE8MZrfLEy1I+vM9rRLGsHLlZ7jmmusi2vr27Q+/34/77nsAO3f+grvvHoNx4+7AihWfAgAefvhx1NbW4u67b8O4cXdg/vy3AAAjR96EsrIyjB07Gi+99C+cc05Oi+e9+OIhSEtLxx13jMLEiQ+gd+/e0mvnnz8Id999Dx57bDzGjbsDjz76EFwuJwBAFEVcd931SElJRY8ePc/6+omIiIiIYpUQaByO6CQqKpwRU80AoKTkIJKTu7Z7X+RyEd6wdI6O6rHHxmPkyFEYPvyqM9ovWu87EVG4jjLPnYioM4uVe7EoCrBYWl4mwpEjalFe3h6MGXMj9Ho9Lr98eLS7Q0RERETUpuIukIFOX58+5+Cjj5ZFuxtERERERO2CI0dERERERESIo+Koky2tinl8v4mIiIioo4mL4kguV8LlquEH9nYSCATgctVALleeemMiIiIiohgRF2uOzGYbqqoccDqr2/W8oijC7+/4aXV/hFyuhNlsi3Y3iIiIiIhOW1wURzKZHFZrSrufN1YiC4mIiIiI6NTiYlodERERERHRqbA4IiIiIiIiQiecVieKQrS7ECHW+kNEFI94LyYiir5YuBefqg9CgBFuREREREREnFZHREREREQEsDgiIiIiIiICwOKIiIiIiIgIAIsjIiIiIiIiACyOiIiIiIiIALA4IiIiIiIiAsDiiIiIiIiICACLIyIiIiIiIgAsjoiIiIiIiACwOCIiIiIiIgIAyKPdgc5mxowZ+OKLL1BUVITly5ejV69e0e4SEVHcqaqqwlNPPYVDhw5BqVSia9eueOGFF5CYmBjtrhERxZXx48ejsLAQoihCq9XiH//4B7Kzs6PdrRYJgUAgEO1OdCZbt25FWloa7rrrLrz++ussjoiIoqC6uhq//vorLrroIgDBL66OHj2Kf/7zn1HuGRFRfDl27BgSEhIAAF999RVyc3PxySefRLlXLeO0ulZOXnoJAAAEk0lEQVQ2aNAgpKSkRLsbRERxzWQySYURAJx33nk4cuRIFHtERBSfGgsjAHA6nRAEIYq9OTVOqyMiok7N7/dj0aJFGD58eLS7QkQUl/72t79h48aNCAQCePvtt6PdnZPiyBEREXVqU6dOhVarxdixY6PdFSKiuDR9+nSsXbsWkyZNwsyZM6PdnZNicURERJ3WjBkzcPDgQbzyyisQRf6TR0QUTTfddBO+//57VFVVRbsrLeK/FERE1Cm9/PLL2LVrF3Jzc6FUKqPdHSKiuONyuVBcXCz9+ZtvvoHRaITJZIpir06OaXWtbNq0afjyyy9RXl4Os9kMk8mElStXRrtbRERx5ffff8cNN9yAzMxMqNVqAEB6ejpyc3Oj3DMiovhRXl6O8ePHo66uDqIowmg04umnn0ZOTk60u9YiFkdERERERETgtDoiIiIiIiIALI6IiIiIiIgAsDgiIiIiIiICwOKIiIiIiIgIAIsjIiIiIiIiACyOiIiIJL1798bBgwej3Q0iIooSebQ7QERE1JLhw4ejvLwcMplMarv55psxefLkKPaKiIg6KxZHREQU015//XUMGTIk2t0gIqI4wGl1RETU4SxduhS33347XnjhBQwcOBAjRozA5s2bpddLS0vx0EMP4cILL8TVV1+Njz76SHrN5/Ph9ddfx1VXXYUBAwZg1KhRKC4ull7ftGkTrrnmGgwaNAjPP/88+Kx0IqL4wZEjIiLqkHbs2IERI0Zgy5YtWLNmDSZOnIivv/4aJpMJjz/+OHr27In169ejoKAA9957LzIyMjB48GDMmzcPK1euxJtvvolu3brh119/hVqtlo67du1aLFmyBE6nE6NGjcIVV1yByy67LIpXSkRE7YUjR0REFNMmTJiAQYMGSf81jgIlJiZi3LhxUCgU+NOf/oRu3bph7dq1KC4uxrZt2/DEE09ApVIhOzsbo0ePxrJlywAAixcvxqOPPoru3btDEAT06dMHZrNZOt/9998Pg8GA1NRUXHTRRcjLy4vKdRMRUfvjyBEREcW03NzcZmuOli5diqSkJAiCILWlpqairKwMZWVlMBqN0Ov1Ea/t2rULAFBSUoIuXbq0eD6bzSb9rNFo4HK5WutSiIgoxnHkiIiIOqTS0tKI9UDFxcWw2+2w2+04evQonE5nxGtJSUkAgOTkZBw6dKjd+0tERLGPxREREXVIlZWVePfdd9HQ0IDPP/8c+fn5GDZsGFJSUjBgwAC8/PLLcLvdyMvLw5IlSzBy5EgAwOjRo/Hqq6/iwIEDCAQCyMvLQ1VVVZSvhoiIYgGn1RERUUx76KGHIp5zNGTIEFx55ZXo378/Dh48iIsvvhhWqxWzZ8+W1g69/PLLmDJlCoYOHQqDwYCHH35Ympp37733wuPx4L777kNVVRW6d++O3NzcqFwbERHFFiHAjFIiIupgli5disWLF2PRokXR7goREXUinFZHREREREQEFkdEREREREQAOK2OiIiIiIgIAEeOiIiIiIiIALA4IiIiIiIiAsDiiIiIiIiICACLIyIiIiIiIgAsjoiIiIiIiAAA/wuLYlO6iMMf8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style='darkgrid')\n",
    "plt.rcParams[\"figure.figsize\"] = (14,8)\n",
    "\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "plt.plot(df_stats['Valid. Accur.'], 'r-o', label=\"Accuracy\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, epochs + 1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
